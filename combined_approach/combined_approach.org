#+PROPERTY: header-args:python :session :results output :exports both
* Outline
- This is an attempt to integrate both proiel_trf & grc_ner into a model, outputting in a .conllu format

** Code - script for _trf, _ner 

*** run_proiel_trf.py
#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_trf")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  os.makedirs(output_folder, exist_ok=True)

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save dependency parsing results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_trf.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_trf processing completed.")

#+end_src

*** run_NER.py

#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_ner")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save NER results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_ner.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_ner processing completed.")
#+end_src
** Code - using babel to switch environments
*** Running proiel_trf - full text
#+BEGIN_SRC python :session proiel_trf :results output :var dataset="1"
import os
os.system("conda run -n proiel_trf python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_proiel_trf.py")
#+END_SRC

*** Running proiel_ner - full text

#+BEGIN_SRC python :session ner :results output :var dataset="1"
import os
os.system("conda run -n ner python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_ner.py")
#+END_SRC

#+RESULTS:
: ‚úÖ NER processing completed.
: 
: /home/gnosis/.conda/envs/ner/lib/python3.9/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
:   with torch.cuda.amp.autocast(self._mixed_precision):

** Merging results in a UD-compliant .conllu-file

*** merge_results.py
**** .json vil ikke merges til en .conllu her :)
#+begin_src python :results output :eval never
  import json
  import os
  import pandas as pd

  input_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results/"
  output_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results_conllu/"

  os.makedirs(output_folder, exist_ok=True)

# Collect all .jsonl files
json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

# Find matching _trf.json and _ner.json files
file_pairs = {}
for filename in json_files:
    base_name = filename.replace("_trf.json", "").replace("_ner.json", "")

    if base_name not in file_pairs:
        file_pairs[base_name] = {}

    if filename.endswith("_trf.json"):
        file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
    elif filename.endswith("_ner.json"):
        file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

# Merge TRF and NER files into CoNLL-U format
for base_name, paths in file_pairs.items():
    if "trf" not in paths or "ner" not in paths:
        print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
        continue

    trf_path = paths["trf"]
    ner_path = paths["ner"]
    conllu_filename = os.path.join(output_folder, base_name + ".conllu")

    with open(trf_path, "r", encoding="utf-8") as trf_file, \
         open(ner_path, "r", encoding="utf-8") as ner_file, \
         open(conllu_filename, "w", encoding="utf-8") as conllu_file:

        conllu_file.write("# This file follows Universal Dependencies format\n\n")

        # Load JSON lines into dictionaries indexed by "id"
        trf_data = {entry["id"]: entry for entry in map(json.loads, trf_file)}
        ner_data = {entry["id"]: entry for entry in map(json.loads, ner_file)}

        # Merge TRF and NER entries based on "id"
        for token_id in sorted(trf_data.keys()):  # Ensure correct order
            trf_entry = trf_data[token_id]
            ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O" if missing

            # ‚úÖ Extract token attributes safely
            form = trf_entry.get("text", "_")
            lemma = trf_entry.get("lemma", "_")
            upos = trf_entry.get("upos", "_")
            xpos = trf_entry.get("xpos", "_")
            feats = trf_entry.get("feats", "_")
            head = trf_entry.get("head", 0)
            deprel = trf_entry.get("dep", "_")
            deps = trf_entry.get("deps", "_")
            ner_label = ner_entry.get("ner", "O")

            # üìå Store Named Entity Label in MISC
            misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

            # üìå Append token data to CoNLL-U format
            conllu_row = [token_id, form, lemma, upos, xpos, feats, head, deprel, deps, misc_field]
            conllu_file.write("\t".join(map(str, conllu_row)) + "\n")

        conllu_file.write("\n")  # Separate sentences with a blank line

print("‚úÖ Merged proiel_trf and NER outputs into CoNLL-U format.")
#+end_src

#+RESULTS:
**** nyt fors√∏g - saves with ID in continuous count - not integer=1

#+begin_src python :results output
   import os
   import json
   import re  # ‚úÖ Regex for flexible filename matching

   input_folder = "/home/gnosis/Documents/au_work/main/corpora/extract/nlp/brent_punct_analysis"
   # debug - check files are actually located in input
   # files = os.listdir(input_folder)
   # print("üìÇ Files in input folder:", files)
   output_folder = "/home/gnosis/Documents/au_work/main/results/brent"

   os.makedirs(output_folder, exist_ok=True)

   # Collect all .json files (assuming they are NDJSON)
   json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

   # Find matching _trf.json and _ner.json files
   file_pairs = {}
   for filename in json_files:
       # Remove `_trf.json` or `_ner.json` to get the base name
       base_name = re.sub(r"(_trf|_ner)\.json$", "", filename)
       print(f"Processing file: {filename} ‚Üí Base name detected: {base_name}")

       if base_name not in file_pairs:
           file_pairs[base_name] = {}

       if filename.endswith("_trf.json"):
           file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
       elif filename.endswith("_ner.json"):
           file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

   # Debugging: Print detected file pairs
   print(f"üîç Detected file pairs: {file_pairs}")

   # Merge TRF and NER files into CoNLL-U format
   for base_name, paths in file_pairs.items():
       if "trf" not in paths or "ner" not in paths:
           print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
           continue

       trf_path = paths["trf"]
       ner_path = paths["ner"]
       conllu_filename = os.path.join(output_folder, base_name + ".conllu")

       with open(trf_path, "r", encoding="utf-8") as trf_file, \
            open(ner_path, "r", encoding="utf-8") as ner_file, \
            open(conllu_filename, "w", encoding="utf-8") as conllu_file:

           conllu_file.write("# This file follows Universal Dependencies format\n\n")

           # Read NDJSON line-by-line
           trf_data = {entry["id"]: entry for entry in map(json.loads, trf_file)}
           ner_data = {entry["id"]: entry for entry in map(json.loads, ner_file)}

           # Debugging: Check if data is being read
           print(f"üìÑ Processing {base_name}: {len(trf_data)} tokens found in TRF")
           print(f"üìÑ Processing {base_name}: {len(ner_data)} tokens found in NER")

           # Merge TRF and NER based on "id"
           for token_id in sorted(trf_data.keys()):  # Ensure correct order
               trf_entry = trf_data[token_id]
               ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O"

               # ‚úÖ Extract token attributes
               form = trf_entry.get("text", "_")
               lemma = trf_entry.get("lemma", "_")
               upos = trf_entry.get("upos", "_")
               xpos = "_"
               feats = trf_entry.get("feats", "_")
               head = trf_entry.get("head", 0)
               deprel = trf_entry.get("dep", "_")
               deps = trf_entry.get("deps", "_")
               ner_label = ner_entry.get("ner", "O")

               # üè∑Ô∏è Store Named Entity Label in MISC
               misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

               # üìå Append token data to CoNLL-U format
               conllu_row = [token_id, form, lemma, upos, xpos, feats, head, deprel, deps, misc_field]
               conllu_file.write("\t".join(map(str, conllu_row)) + "\n")

           conllu_file.write("\n")  # Separate sentences with a blank line

   print("‚úÖ Merged proiel_trf and NER outputs into CoNLL-U format.")
#+end_src

**** UD-compliant, integer=1 - however, deps and so on are fucked due to proiel_trf, so left blank in order to focus on annotation of UPOS and NER

#+begin_src python :results output
  import os
  import json
  import re  # ‚úÖ Regex for flexible filename matching

  input_folder = "/home/gnosis/Documents/au_work/main/results/u09/SBLGNT_punct_analysis"
  # debug - check files are actually located in input
  # files = os.listdir(input_folder)
  # print("üìÇ Files in input folder:", files)
  output_folder = "/home/gnosis/Documents/au_work/main/results/sblgnt_ud_comp"

  # ‚úÖ Ensure output directory exists
  os.makedirs(output_folder, exist_ok=True)


  # ‚úÖ Collect all JSON files (assuming they are NDJSON)
  json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

  # ‚úÖ Detect file pairs (_trf.json and _ner.json)
  file_pairs = {}
  for filename in json_files:
      base_name = re.sub(r"(_trf|_ner)\.json$", "", filename)  # Remove suffix
      print(f"Processing file: {filename} ‚Üí Base name detected: {base_name}")

      if base_name not in file_pairs:
          file_pairs[base_name] = {}

      if filename.endswith("_trf.json"):
          file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
      elif filename.endswith("_ner.json"):
          file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

  # ‚úÖ Debug: Show detected file pairs
  print(f"üîç Detected file pairs: {file_pairs}")

  # ‚úÖ Process each file pair
  for base_name, paths in file_pairs.items():
      if "trf" not in paths or "ner" not in paths:
          print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
          continue

      trf_path = paths["trf"]
      ner_path = paths["ner"]
      conllu_filename = os.path.join(output_folder, base_name + ".conllu")

      with open(trf_path, "r", encoding="utf-8") as trf_file, \
           open(ner_path, "r", encoding="utf-8") as ner_file, \
           open(conllu_filename, "w", encoding="utf-8") as conllu_file:

          conllu_file.write("# This file follows Universal Dependencies format\n\n")

          # ‚úÖ Read NDJSON into dictionaries
          trf_data = {entry["id"]: entry for entry in map(json.loads, trf_file)}
          ner_data = {entry["id"]: entry for entry in map(json.loads, ner_file)}

          # ‚úÖ Debugging: Check token count
          print(f"üìÑ Processing {base_name}: {len(trf_data)} tokens in TRF")
          print(f"üìÑ Processing {base_name}: {len(ner_data)} tokens in NER")

          # ‚úÖ Track sentence ID
          sentence_id = 1
          sentence_tokens = []
          sentence_text = []

          # ‚úÖ Sort tokens by numeric ID (to prevent ID mismatch)
          sorted_tokens = sorted(trf_data.keys(), key=lambda x: int(x))

          for token_id in sorted_tokens:
              trf_entry = trf_data[token_id]
              ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O"

              # ‚úÖ Extract token attributes
              form = trf_entry.get("text", "_")
              lemma = trf_entry.get("lemma", "_")
              upos = trf_entry.get("upos", "_")  # Get UPOS tag
              xpos = "_"
              feats = trf_entry.get("feats", "_")
              ner_label = ner_entry.get("ner", "O")

              # ‚úÖ Named Entity Tagging in MISC Field
              misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

              # ‚úÖ Start a new sentence when encountering a ROOT (HEAD=0)
              if len(sentence_tokens) > 0 and trf_entry.get("head", 0) == 0:
                  # Write previous sentence
                  conllu_file.write(f"# sent_id = {sentence_id}\n")
                  conllu_file.write(f"# text = {' '.join(sentence_text)}\n")
                  conllu_file.write("\n".join(sentence_tokens) + "\n\n")

                  # ‚úÖ Reset sentence buffers for the new sentence
                  sentence_id += 1
                  sentence_tokens = []
                  sentence_text = []

              # ‚úÖ Add token to sentence buffer (head and deprel removed)
              sentence_tokens.append("\t".join([
                  str(len(sentence_tokens) + 1),  # Token index within sentence
                  form, lemma, upos, xpos, feats, "_", "_", "_", misc_field
              ]))
              sentence_text.append(form)  # Store original text for `# text = ...`

          # ‚úÖ Write any remaining sentence at the end
          if sentence_tokens:
              conllu_file.write(f"# sent_id = {sentence_id}\n")
              conllu_file.write(f"# text = {' '.join(sentence_text)}\n")
              conllu_file.write("\n".join(sentence_tokens) + "\n\n")

  print("‚úÖ Merged proiel_trf and NER outputs into CoNLL-U format without dependencies.")
#+end_src
