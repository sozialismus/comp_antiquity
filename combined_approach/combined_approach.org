#+PROPERTY: header-args:python :session :results output :exports both
* Outline
- This is an attempt to integrate both proiel_trf & grc_ner into a model, outputting in a .conllu format

** Code - script for _trf, _ner 

*** run_proiel_trf.py
#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_trf")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  os.makedirs(output_folder, exist_ok=True)

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save dependency parsing results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_trf.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_trf processing completed.")

#+end_src

*** run_NER.py

#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_ner")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save NER results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_ner.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_ner processing completed.")
#+end_src
** Code - using babel to switch environments
*** Running proiel_trf - full text
#+BEGIN_SRC python :session proiel_trf :results output :var dataset="1"
import os
os.system("conda run -n proiel_trf python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_proiel_trf.py")
#+END_SRC

*** Running proiel_ner - full text

#+BEGIN_SRC python :session ner :results output :var dataset="1"
import os
os.system("conda run -n ner python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_ner.py")
#+END_SRC

#+RESULTS:
: ‚úÖ NER processing completed.
: 
: /home/gnosis/.conda/envs/ner/lib/python3.9/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
:   with torch.cuda.amp.autocast(self._mixed_precision):

** Merging results in a UD-compliant .conllu-file

*** merge_results.py
**** .json vil ikke merges til en .conllu her :)
#+begin_src python :results output :eval never
  import json
  import os
  import pandas as pd

  input_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results/"
  output_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results_conllu/"

  os.makedirs(output_folder, exist_ok=True)

# Collect all .jsonl files
json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

# Find matching _trf.json and _ner.json files
file_pairs = {}
for filename in json_files:
    base_name = filename.replace("_trf.json", "").replace("_ner.json", "")

    if base_name not in file_pairs:
        file_pairs[base_name] = {}

    if filename.endswith("_trf.json"):
        file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
    elif filename.endswith("_ner.json"):
        file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

# Merge TRF and NER files into CoNLL-U format
for base_name, paths in file_pairs.items():
    if "trf" not in paths or "ner" not in paths:
        print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
        continue

    trf_path = paths["trf"]
    ner_path = paths["ner"]
    conllu_filename = os.path.join(output_folder, base_name + ".conllu")

    with open(trf_path, "r", encoding="utf-8") as trf_file, \
         open(ner_path, "r", encoding="utf-8") as ner_file, \
         open(conllu_filename, "w", encoding="utf-8") as conllu_file:

        conllu_file.write("# This file follows Universal Dependencies format\n\n")

        # Load JSON lines into dictionaries indexed by "id"
        trf_data = {entry["id"]: entry for entry in map(json.loads, trf_file)}
        ner_data = {entry["id"]: entry for entry in map(json.loads, ner_file)}

        # Merge TRF and NER entries based on "id"
        for token_id in sorted(trf_data.keys()):  # Ensure correct order
            trf_entry = trf_data[token_id]
            ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O" if missing

            # ‚úÖ Extract token attributes safely
            form = trf_entry.get("text", "_")
            lemma = trf_entry.get("lemma", "_")
            upos = trf_entry.get("upos", "_")
            xpos = trf_entry.get("xpos", "_")
            feats = trf_entry.get("feats", "_")
            head = trf_entry.get("head", 0)
            deprel = trf_entry.get("dep", "_")
            deps = trf_entry.get("deps", "_")
            ner_label = ner_entry.get("ner", "O")

            # üìå Store Named Entity Label in MISC
            misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

            # üìå Append token data to CoNLL-U format
            conllu_row = [token_id, form, lemma, upos, xpos, feats, head, deprel, deps, misc_field]
            conllu_file.write("\t".join(map(str, conllu_row)) + "\n")

        conllu_file.write("\n")  # Separate sentences with a blank line

print("‚úÖ Merged proiel_trf and NER outputs into CoNLL-U format.")
#+end_src

#+RESULTS:
**** nyt fors√∏g - saves with ID in continuous count - not integer=1

#+begin_src python :results output
import os
import json
import re  # ‚úÖ Regex for flexible filename matching

input_folder = "/home/gnosis/Documents/au_work/main/results/u10/bjarke/SBLGNT_punct_analysis/"
# debug - check files are actually located in input
# files = os.listdir(input_folder)
# print("üìÇ Files in input folder:", files)
output_folder = "/home/gnosis/Documents/au_work/main/results/u10/bjarke/SBLGNT_punct_analysis_conllu"

os.makedirs(output_folder, exist_ok=True)

# Collect all .json files (assuming they are NDJSON)
json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

# Find matching _trf.json and _ner.json files
file_pairs = {}
for filename in json_files:
    # Remove `_trf.json` or `_ner.json` to get the base name
    base_name = re.sub(r"(_trf|_ner)\.json$", "", filename)
    print(f"Processing file: {filename} ‚Üí Base name detected: {base_name}")

    if base_name not in file_pairs:
        file_pairs[base_name] = {}

    if filename.endswith("_trf.json"):
        file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
    elif filename.endswith("_ner.json"):
        file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

# Debugging: Print detected file pairs
print(f"üîç Detected file pairs: {file_pairs}")

# Merge TRF and NER files into CoNLL-U format
for base_name, paths in file_pairs.items():
    if "trf" not in paths or "ner" not in paths:
        print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
        continue

    trf_path = paths["trf"]
    ner_path = paths["ner"]
    conllu_filename = os.path.join(output_folder, base_name + ".conllu")

    with open(trf_path, "r", encoding="utf-8") as trf_file, \
         open(ner_path, "r", encoding="utf-8") as ner_file, \
         open(conllu_filename, "w", encoding="utf-8") as conllu_file:

        conllu_file.write("# This file follows Universal Dependencies format\n\n")

        # Read NDJSON line-by-line
        trf_data = {entry["global_id"]: entry for entry in map(json.loads, trf_file)}
        ner_data = {entry["global_id"]: entry for entry in map(json.loads, ner_file)}

        # Debugging: Check if data is being read
        print(f"üìÑ Processing {base_name}: {len(trf_data)} tokens found in TRF")
        print(f"üìÑ Processing {base_name}: {len(ner_data)} tokens found in NER")
        # ‚úÖ Debugging: Check for missing tokens in either file
        trf_ids = set(trf_data.keys())
        ner_ids = set(ner_data.keys())

        missing_in_ner = trf_ids - ner_ids
        missing_in_trf = ner_ids - trf_ids

        if missing_in_ner:
            print(f"‚ö†Ô∏è Tokens in TRF but missing in NER: {sorted(missing_in_ner)[:10]} ...")  # Show first 10
        if missing_in_trf:
            print(f"‚ö†Ô∏è Tokens in NER but missing in TRF: {sorted(missing_in_trf)[:10]} ...")  # Show first 10

        # ‚úÖ Debugging: Check for mismatched token texts
        for token_id in sorted(trf_ids & ner_ids):  # Only check IDs that exist in both
            if trf_data[token_id]["text"] != ner_data[token_id]["text"]:
                print(f"‚ö†Ô∏è Text mismatch at ID {token_id}: TRF='{trf_data[token_id]['text']}' vs NER='{ner_data[token_id]['text']}'")
                break  # Stop at first mismatch        # Merge TRF and NER based on "id"
        for token_id in sorted(trf_data.keys()):  # Ensure correct order
            trf_entry = trf_data[token_id]
            ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O"

            # ‚úÖ Extract token attributes
            form = trf_entry.get("text", "_")
            lemma = trf_entry.get("lemma", "_")
            upos = trf_entry.get("upos", "_")
            xpos = "_"
            feats = trf_entry.get("feats", "_")
            head = trf_entry.get("head", 0)
            deprel = trf_entry.get("dep", "_")
            deps = trf_entry.get("deps", "_")
            ner_label = ner_entry.get("ner", "O")

            # üè∑Ô∏è Store Named Entity Label in MISC
            misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

            # üìå Append token data to CoNLL-U format
            conllu_row = [token_id, form, lemma, upos, xpos, feats, head, deprel, deps, misc_field]
            conllu_file.write("\t".join(map(str, conllu_row)) + "\n")

        conllu_file.write("\n")  # Separate sentences with a blank line

print("‚úÖ Merged proiel_trf and NER outputs into CoNLL-U format.")
#+end_src



**** UD-compliant, integer=1 - however, deps and so on are fucked due to proiel_trf, so left blank in order to focus on annotation of UPOS and NER

#+begin_src python :results output
  import os
  import json
  import re  # ‚úÖ Regex for flexible filename matching

  input_folder = "/home/gnosis/Documents/au_work/main/results/u09/try_analysis"
  # debug - check files are actually located in input
  # files = os.listdir(input_folder)
  # print("üìÇ Files in input folder:", files)
  output_folder = "/home/gnosis/Documents/au_work/main/results/u09/try_conllu"

  # ‚úÖ Ensure output directory exists
  os.makedirs(output_folder, exist_ok=True)


  # ‚úÖ Collect all JSON files (assuming they are NDJSON)
  json_files = [f for f in os.listdir(input_folder) if f.endswith(".json")]

  # ‚úÖ Detect file pairs (_trf.json and _ner.json)
  file_pairs = {}
  for filename in json_files:
      base_name = re.sub(r"(_trf|_ner)\.json$", "", filename)  # Remove suffix
      # print(f"Processing file: {filename} ‚Üí Base name detected: {base_name}")

      if base_name not in file_pairs:
          file_pairs[base_name] = {}

      if filename.endswith("_trf.json"):
          file_pairs[base_name]["trf"] = os.path.join(input_folder, filename)
      elif filename.endswith("_ner.json"):
          file_pairs[base_name]["ner"] = os.path.join(input_folder, filename)

  # ‚úÖ Debug: Show detected file pairs
  # print(f"üîç Detected file pairs: {file_pairs}")

  # ‚úÖ Process each file pair
  for base_name, paths in file_pairs.items():
      if "trf" not in paths or "ner" not in paths:
          # print(f"‚ö†Ô∏è Warning: Missing TRF or NER file for {base_name}, skipping merge.")
          continue

      trf_path = paths["trf"]
      ner_path = paths["ner"]
      conllu_filename = os.path.join(output_folder, base_name + ".conllu")

      with open(trf_path, "r", encoding="utf-8") as trf_file, \
           open(ner_path, "r", encoding="utf-8") as ner_file, \
           open(conllu_filename, "w", encoding="utf-8") as conllu_file:

          # ‚úÖ Add UD-compliant metadata at the beginning
          conllu_file.write("# This file follows Universal Dependencies format\n")
          conllu_file.write(f"# newdoc id = {base_name}\n")  # Document ID from base_name
          conllu_file.write("# global.features = syntax_not_annotated\n\n")

          # ‚úÖ Read NDJSON into dictionaries
          trf_data = {entry["id"]: entry for entry in map(json.loads, trf_file)}
          ner_data = {entry["id"]: entry for entry in map(json.loads, ner_file)}

          # ‚úÖ Debugging: Check token count
          print(f"üìÑ Processing {base_name}: {len(trf_data)} tokens in TRF")
          print(f"üìÑ Processing {base_name}: {len(ner_data)} tokens in NER")

          # ‚úÖ Track sentence ID
          sentence_id = 1
          sentence_tokens = []
          sentence_text = []

          # ‚úÖ Sort tokens by numeric ID (to prevent ID mismatch)
          sorted_tokens = sorted(trf_data.keys(), key=lambda x: int(x))

          for token_id in sorted_tokens:
              trf_entry = trf_data[token_id]
              ner_entry = ner_data.get(token_id, {"ner": "O"})  # Default to "O"

              # ‚úÖ Extract token attributes
              form = trf_entry.get("text", "_")
              lemma = trf_entry.get("lemma", "_")
              upos = trf_entry.get("upos", "_")  # Get UPOS tag
              xpos = "_"
              feats = trf_entry.get("feats", "_")
              ner_label = ner_entry.get("ner", "O")

              # ‚úÖ Named Entity Tagging in MISC Field
              misc_field = f"NER={ner_label}" if ner_label != "O" else "_"

              # ‚úÖ Start a new sentence when encountering a ROOT (HEAD=0)
              if len(sentence_tokens) > 0 and trf_entry.get("head", 0) == 0:
                  # Write previous sentence
                  conllu_file.write(f"# sent_id = {sentence_id}\n")
                  conllu_file.write(f"# text = {' '.join(sentence_text)}\n")
                  conllu_file.write("\n".join(sentence_tokens) + "\n\n")

                  # ‚úÖ Reset sentence buffers for the new sentence
                  sentence_id += 1
                  sentence_tokens = []
                  sentence_text = []

              # ‚úÖ Add token to sentence buffer (head and deprel removed)
              sentence_tokens.append("\t".join([
                  str(len(sentence_tokens) + 1),  # Token index within sentence
                  form, lemma, upos, xpos, feats, "_", "_", "_", misc_field
              ]))
              sentence_text.append(form)  # Store original text for `# text = ...`

          # ‚úÖ Write any remaining sentence at the end
          if sentence_tokens:
              conllu_file.write(f"# sent_id = {sentence_id}\n")
              conllu_file.write(f"# text = {' '.join(sentence_text)}\n")
              conllu_file.write("\n".join(sentence_tokens) + "\n\n")

      print(f"‚úÖ Processed {base_name}: CoNLL-U file saved with 'syntax_not_annotated' flag.")

#+end_src

#+RESULTS:
: üìÑ Processing John_nlp_ready: 18003 tokens in TRF
: üìÑ Processing John_nlp_ready: 18003 tokens in NER
: ‚úÖ Processed John_nlp_ready: CoNLL-U file saved with 'syntax_not_annotated' flag.

** Extract various entities from finished .conllu and export to either .csv or .json

*** Script for extraction from .conllu, keeping ID, FORM, LEMMA, UPOS and NER (MISC)
#+begin_src python :results output
  import os
  import csv
  import re

  # ‚úÖ Define input and base output folder (change as needed)
  INPUT_PATH = "/home/gnosis/Documents/au_work/main/results/u10/bjarke"  # Can be a file or directory
  OUTPUT_BASE = "/home/gnosis/Documents/au_work/main/results/u10/bjarke_ner_focused "  # Parent directory for all output files

  def extract_conllu_to_csv(conllu_file, input_base):
      """Extracts relevant fields from a CoNLL-U file and saves as a CSV in a parallel structure."""
      
      # ‚úÖ Determine relative path and new output location
      relative_path = os.path.relpath(conllu_file, input_base)  # Get relative path from input root
      output_csv = os.path.join(OUTPUT_BASE, relative_path.replace(".conllu", ".csv"))  # Change extension

      # ‚úÖ Ensure the output directory exists
      os.makedirs(os.path.dirname(output_csv), exist_ok=True)

      extracted_data = []
      with open(conllu_file, "r", encoding="utf-8") as file:
          for line in file:
              line = line.strip()
              if not line or line.startswith("#"):
                  continue  # Skip comments and empty lines

              columns = line.split("\t")
              if len(columns) < 10:
                  continue  # Skip malformed lines

              word_id, form, lemma, upos, misc = columns[0], columns[1], columns[2], columns[3], columns[9]
              ner_match = re.search(r"NER=([^\s|]+)", misc)
              ner = ner_match.group(1) if ner_match else "O"  # Default to "O"

              extracted_data.append([word_id, form, lemma, upos, ner])

      # ‚úÖ Write to CSV
      with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
          writer = csv.writer(csvfile)
          writer.writerow(["ID", "FORM", "LEMMA", "UPOS", "NER"])  # CSV Header
          writer.writerows(extracted_data)

      print(f"‚úÖ Processed: {conllu_file} ‚Üí {output_csv}")


  def process_directory(input_folder):
      """Processes all .conllu files in a directory and saves CSVs in the parallel output structure."""
      conllu_files = []
      for root, _, files in os.walk(input_folder):
          for file in files:
              if file.endswith(".conllu"):
                  conllu_files.append(os.path.join(root, file))

      if not conllu_files:
          print("‚ö†Ô∏è No .conllu files found in the directory.")
          return

      for conllu_path in conllu_files:
          extract_conllu_to_csv(conllu_path, input_folder)

      print(f"‚úÖ Batch processing completed. CSV files saved in {OUTPUT_BASE}")


  # ‚úÖ Automatically detect if processing a file or directory
  if os.path.isdir(INPUT_PATH):
      process_directory(INPUT_PATH)
  else:
      extract_conllu_to_csv(INPUT_PATH, os.path.dirname(INPUT_PATH))

#+end_src


* Post-processing

** Analyzing NER-performance
- Making a script for analyzing the overall performance of the NER-model
  - Leverages the lemmatization of the TRF model, and analyzes the normalized lemmas/morphemes here, whereby the less often used words in NT/LXX (eg. names like ŒôŒ∑œÉœÖœÇ are only reckognized in the nominative case)
    - Thereby generating an overall hit/miss ratio, annotating the original .conllu or extrated entities from a .conllu (the .csv files from above)

*** The actual script

**** Updated NER_compare

#+begin_src python
  import os
  import csv
  import re
  import spacy
  from collections import Counter

  # ‚úÖ Load the correct Greek NER model (grc_ner_trf)
  nlp = spacy.load("grc_ner_trf")

  # ‚úÖ Define input path
  INPUT_PATH = "/home/gnosis/Documents/au_work/main/results/u10/bjarke/conllu/SBLGNT_punct_analysis_conllu"  # Adjust as needed

  def run_ner_on_lemmatized_text(lemmas):
      """Reconstruct text from lemmas and run NER using grc_ner_trf."""
      text = " ".join(lemmas)  # ‚úÖ Rebuild the full text from lemmas
      doc = nlp(text)  # ‚úÖ Process with the correct model
      return [token.ent_type_ if token.ent_type_ else "O" for token in doc]  # ‚úÖ Extract entity labels

  def process_csv(csv_file):
      """Analyze NER performance using lemmata in a CSV file and update it if needed."""
      total_original = Counter()
      total_lemma_ner = Counter()
      updated_rows = []

      output_file = csv_file  # ‚úÖ Overwrite the same file

      # ‚úÖ Read CSV
      with open(csv_file, "r", encoding="utf-8") as file:
          reader = csv.DictReader(file)
          fieldnames = reader.fieldnames + ["NER_FROM_LEMMA"]  # ‚úÖ Append new column

          lemmas = [row["LEMMA"] for row in reader]  # ‚úÖ Collect all lemmas
          file.seek(0)
          next(reader)  # ‚úÖ Skip header

          predicted_ner = run_ner_on_lemmatized_text(lemmas)  # ‚úÖ Run NER on the full sequence

          for i, row in enumerate(reader):
              original_ner = row["NER"]
              lemma_ner = predicted_ner[i]  # ‚úÖ Get corresponding lemma-based NER

              total_original[original_ner] += 1
              total_lemma_ner[lemma_ner] += 1

              # ‚úÖ Only add "NER_FROM_LEMMA" if different from original
              row["NER_FROM_LEMMA"] = lemma_ner if original_ner != lemma_ner else ""

              updated_rows.append(row)

      # ‚úÖ Write corrected CSV
      with open(output_file, "w", encoding="utf-8", newline="") as file:
          writer = csv.DictWriter(file, fieldnames=fieldnames, extrasaction="ignore", delimiter=",")
          writer.writeheader()
          writer.writerows(updated_rows)

      return total_original, total_lemma_ner

  def process_conllu(conllu_file):
      """Analyze NER performance using lemmata in a CoNLL-U file and update it if needed."""
      total_original = Counter()
      total_lemma_ner = Counter()
      updated_lines = []

      lemmas = []
      with open(conllu_file, "r", encoding="utf-8") as file:
          for line in file:
              stripped_line = line.strip()
              if not stripped_line or stripped_line.startswith("#"):
                  updated_lines.append(line)
                  continue

              columns = stripped_line.split("\t")
              if len(columns) < 10:
                  updated_lines.append(line)
                  continue

              lemmas.append(columns[2])  # ‚úÖ Collect lemmas in order

      # ‚úÖ Run NER on the full lemmatized text
      predicted_ner = run_ner_on_lemmatized_text(lemmas)

      with open(conllu_file, "r", encoding="utf-8") as file:
          index = 0
          for line in file:
              stripped_line = line.strip()
              if not stripped_line or stripped_line.startswith("#"):
                  updated_lines.append(line)
                  continue

              columns = stripped_line.split("\t")
              if len(columns) < 10:
                  updated_lines.append(line)
                  continue

              misc = columns[9]
              ner_match = re.search(r"NER=([^\s|]+)", misc)
              original_ner = ner_match.group(1) if ner_match else "O"
              lemma_ner = predicted_ner[index]  # ‚úÖ Get the NER prediction for this token

              total_original[original_ner] += 1
              total_lemma_ner[lemma_ner] += 1

              # ‚úÖ Only add "NER_FROM_LEMMA" if different from original
              if original_ner != lemma_ner:
                  if misc == "_":
                      columns[9] = f"NER_FROM_LEMMA={lemma_ner}"
                  else:
                      columns[9] += f"|NER_FROM_LEMMA={lemma_ner}"

              updated_lines.append("\t".join(columns) + "\n")
              index += 1

      # ‚úÖ Overwrite CoNLL-U file
      with open(conllu_file, "w", encoding="utf-8") as file:
          file.writelines(updated_lines)

      return total_original, total_lemma_ner

  def analyze_files(input_path):
      """Determine file type (.csv or .conllu), run NER on lemmas, and update the files."""
      if os.path.isdir(input_path):
          files = [os.path.join(root, f) for root, _, filenames in os.walk(input_path) for f in filenames if f.endswith((".csv", ".conllu"))]
      else:
          files = [input_path]

      total_original = Counter()
      total_lemma_ner = Counter()

      for file in files:
          print(f"üìä Processing: {file}")
          if file.endswith(".csv"):
              orig, lemma_ner = process_csv(file)
          elif file.endswith(".conllu"):
              orig, lemma_ner = process_conllu(file)
          else:
              continue

          total_original.update(orig)
          total_lemma_ner.update(lemma_ner)

      # ‚úÖ Compute differences in NER detection
      print("\nüìà **NER Performance Comparison:**")
      print(f"Total Original Entities: {sum(total_original.values())}")
      print(f"Total Entities from Lemmas: {sum(total_lemma_ner.values())}")

      # ‚úÖ Show individual category breakdown
      print("\nüîç **Breakdown per Entity Type:**")
      print(f"{'Entity':<20}{'Original':<10}{'From Lemma':<10}{'Difference':<10}")
      for entity in set(total_original.keys()).union(total_lemma_ner.keys()):
          original_count = total_original.get(entity, 0)
          lemma_count = total_lemma_ner.get(entity, 0)
          diff = lemma_count - original_count
          print(f"{entity:<20}{original_count:<10}{lemma_count:<10}{diff:<10}")

      print("\n‚úÖ Analysis Complete!")

  # ‚úÖ Run analysis
  analyze_files(INPUT_PATH)
#+end_src




* Difference in document_id's

#+begin_src python :results output
  import pandas as pd
  import os
  from pathlib import Path

  # File paths (adjust as necessary)
  legacy_file = "/home/gnosis/Documents/au_work/main/comp_antiquity/dat/greek/parsed_data/index_legacy.csv"
  new_file = "/home/gnosis/Documents/au_work/main/comp_antiquity/dat/greek/parsed_data/index_new_without_manu.csv"
  output_csv = "/home/gnosis/Documents/au_work/main/comp_antiquity/change/changed_document_ids.csv"

  # Read the CSV files
  legacy_df = pd.read_csv(legacy_file)
  new_df = pd.read_csv(new_file)

  # Function to filter out document_ids starting with "SEPA_" or "SEP_"
  def filter_doc_ids(df):
      mask = ~df["document_id"].str.startswith("SEPA_") & ~df["document_id"].str.startswith("SEP_")
      return df[mask]

  # Filter both DataFrames
  legacy_df = filter_doc_ids(legacy_df)
  new_df = filter_doc_ids(new_df)

  # Merge the two DataFrames on document_id using an outer join
  # The _merge column will indicate which side the row comes from:
  # "both" means the document_id exists in both; "left_only" or "right_only" indicates a mismatch.
  merged_df = pd.merge(
      legacy_df, new_df,
      on="document_id",
      how="outer",
      indicator=True,
      suffixes=('_legacy', '_new')
  )

  # Identify rows where the document_id is not present in both files
  changed_df = merged_df[merged_df["_merge"] != "both"]
  num_changed = len(changed_df)

  print(f"Number of document IDs that are not the exact same: {num_changed}")

  # Ensure the output directory exists
  Path(output_csv).parent.mkdir(parents=True, exist_ok=True)

  # Save the results to a CSV file
  changed_df.to_csv(output_csv, index=False, encoding="utf-8")
  print(f"Results saved to {output_csv}")
#+end_src

