#+PROPERTY: header-args:python :session :results output :exports both
* Outline
- This is an attempt to integrate both proiel_trf & grc_ner into a model, outputting in a .conllu format

** Code - script for _trf, _ner 

*** run_proiel_trf.py
#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_trf")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  os.makedirs(output_folder, exist_ok=True)

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save dependency parsing results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_trf.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_trf processing completed.")

#+end_src

*** run_NER.py

#+begin_src python :results output :eval never
  import spacy
  import json
  import os

  nlp = spacy.load("proiel_ner")

  input_folder = "/path/to/corpus/"
  output_folder = "/path/to/results/"

  for filename in os.listdir(input_folder):
      if filename.endswith(".txt"):
          file_path = os.path.join(input_folder, filename)
          with open(file_path, "r", encoding="utf-8") as f:
              text = f.read()

          doc = nlp(text)

          # Save NER results
          output_path = os.path.join(output_folder, filename.replace(".txt", "_ner.json"))
          with open(output_path, "w", encoding="utf-8") as f:
              json.dump(doc.to_json(), f, ensure_ascii=False, indent=4)

  print("proiel_ner processing completed.")
#+end_src
** Code - using babel to switch environments
*** Running proiel_trf
#+BEGIN_SRC python :session proiel_trf :results output
import os
os.system("conda run -n proiel_trf python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_proiel_trf.py")
#+END_SRC

#+RESULTS:
: /home/gnosis/.conda/envs/proiel_trf/lib/python3.10/site-packages/spacy_transformers/layers/hf_shim.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
:   self._model.load_state_dict(torch.load(filelike, map_location=device))
: /tmp/tmppadj8bmg: line 3: 495217 Killed                  python /home/gnosis/Documents/au_work/main/comp_antiquity/combined_approach/run_proiel_trf.py
: 
: /usr/lib/python3.13/site-packages/conda/gateways/logging.py:66: FutureWarning: functools.partial will be a method descriptor in future Python versions; wrap it in staticmethod() if you want to preserve the old behavior
:   record.msg = self.TOKEN_REPLACE(record.msg)
: ERROR conda.cli.main_run:execute(125): `conda run python /home/gnosis/Documents/au_work/main/comp_antiquity/combined_approach/run_proiel_trf.py` failed. (See above for error)


*** Running proiel_ner 

#+BEGIN_SRC python :session ner :results output
import os
os.system("conda run -n ner python ~/Documents/au_work/main/comp_antiquity/combined_approach/run_ner.py")
#+END_SRC

#+RESULTS:
: ‚úÖ NER processing completed.
: 
: /home/gnosis/.conda/envs/ner/lib/python3.9/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
:   with torch.cuda.amp.autocast(self._mixed_precision):

** Merging results in a UD-compliant .conllu-file

*** merge_results.py

#+begin_src python :results output
  import json
  import os
  import pandas as pd

  input_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results/"
  output_folder = "~/Documents/au_work/main/corpora/trf_ner_v2_results_conllu/"

  os.makedirs(output_folder, exist_ok=True)

  for filename in os.listdir(input_folder):
    if filename.endswith("_trf.json"):
        base_name = filename.replace("_trf.json", "")
        trf_path = os.path.join(input_folder, filename)
        ner_path = os.path.join(input_folder, base_name + "_ner.json")

        # Load dependency parsing data (proiel_trf)
        with open(trf_path, "r", encoding="utf-8") as f:
            trf_data = json.load(f)

        # Load Named Entity Recognition (NER) results
        with open(ner_path, "r", encoding="utf-8") as f:
            ner_data = json.load(f)

        # Validate token structure
        if not isinstance(trf_data.get("tokens"), list):
            print(f"Warning: 'tokens' missing or not a list in {trf_path}")
            continue  # Skip this file

        # üìå Extract Named Entities and store in a dictionary
        ner_entities = {}
        for ent in ner_data.get("ents", []):
            for i in range(ent["start"], ent["end"]):  # Assign NER labels to token indices
                ner_entities[i] = ent["label"]

        # üõ† Construct CoNLL-U format data
        conllu_data = []
        for i, token in enumerate(trf_data["tokens"]):
            # ‚úÖ Extract token attributes safely (use "_" as default for missing values)
            token_id = i + 1  # CoNLL-U ID (1-based index)
            form = token.get("text", token.get("orth", token.get("form", "_")))  # Word form
            lemma = token.get("lemma", "_")  # Lemma
            upos = token.get("upos", "_")  # Universal POS
            xpos = "_"  # XPOS (not available)
            feats = token.get("feats", "_")  # Morphological features
            head = token.get("head", -1) + 1 if token.get("head", -1) != i else 0  # Root is 0
            deprel = token.get("dep", "_")  # Dependency relation
            deps = "_"  # Enhanced dependencies (not available)

            # üè∑Ô∏è Named Entity Label (stored in MISC only if present)
            ner_label = ner_entities.get(i, None)
            misc_field = f"ner={ner_label}" if ner_label else "_"

            # üìå Append token data to CoNLL-U format
            conllu_data.append([
                token_id, form, lemma, upos, xpos, feats, head, deprel, deps, misc_field
            ])

        # üìù Save CoNLL-U file
        conllu_filename = os.path.join(output_folder, base_name + ".conllu")
        with open(conllu_filename, "w", encoding="utf-8") as f:
            f.write("# This file follows Universal Dependencies format\n\n")
            for row in conllu_data:
                f.write("\t".join(map(str, row)) + "\n")
            f.write("\n")

print("‚úÖ CoNLL-U files saved successfully.")
#+end_src

#+RESULTS:
: CoNLL-U files saved.

