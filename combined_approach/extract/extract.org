* Extracting text
- The following code will be for the purpose of extracting text from various sources
** SBLGNT
- From [[https://github.com/LogosBible/SBLGNT][Github, Logos, SBLGNT text]]
*** Script for extraction - already lovely close-reading friendly text
**** No punct
#+begin_src python
import xml.etree.ElementTree as ET
import re
import os

# Define input and output directories
input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"  # Change this to your actual directory path
output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_nopunct"

# Ensure output directory exists
os.makedirs(output_dir, exist_ok=True)

# Function to process a single XML file
def process_xml_file(file_path):
    tree = ET.parse(file_path)
    root = tree.getroot()
    
    nlp_text = []
    
    for paragraph in root.findall(".//p"):
        verse_text = []
        for word in paragraph.findall(".//w"):
            verse_text.append(word.text.strip() if word.text else "")
        
        full_text = " ".join(verse_text)
        nlp_text_clean = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s]", "", full_text)  # Keep only Greek characters
        
        nlp_text.append(nlp_text_clean)
    
    # Extract filename without extension
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Save NLP-ready text
    with open(os.path.join(output_dir, f"{base_name}_nlp_ready.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(nlp_text))

# Iterate over all XML files in the directory
for file_name in os.listdir(input_dir):
    if file_name.endswith(".xml"):
        file_path = os.path.join(input_dir, file_name)
        print(f"Processing {file_name}...")
        process_xml_file(file_path)

print("Processing complete. Files saved in:", output_dir)
#+end_src

#+RESULTS:
: None
**** Punctuation included - by paragraph in .xml

#+begin_src python
  import xml.etree.ElementTree as ET
  import re
  import os

  # Define input and output directories
  input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"  # Change this to your actual directory path
  output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct"
  # Ensure output directory exists
  os.makedirs(output_dir, exist_ok=True)

  # Function to process a single XML file
  def process_xml_file(file_path):
      tree = ET.parse(file_path)
      root = tree.getroot()
      
      nlp_text = []
      
      for paragraph in root.findall(".//p"):
          verse_text = []

          for element in paragraph:
              if element.tag == "prefix" or element.tag == "suffix":
                  text = element.text.strip() if element.text else ""
                  if text:
                      # Attach punctuation **without adding extra spaces**
                      verse_text.append(text)
              elif element.tag == "w":
                  text = element.text.strip() if element.text else ""
                  if text:
                      # Append words with a space **before** them
                      if verse_text:  # Only add a space if this is not the first word
                          verse_text.append(" ")
                      verse_text.append(text)

          # Join everything into a properly spaced sentence
          full_text = "".join(verse_text).strip()

          # Updated regex: keep Greek letters, spaces, and punctuation marks (.,;¬∑)
          nlp_text_clean = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", full_text)

          nlp_text.append(nlp_text_clean)
          
      # Extract filename without extension
      base_name = os.path.splitext(os.path.basename(file_path))[0]
      
      # Save NLP-ready text
      with open(os.path.join(output_dir, f"{base_name}_nlp_ready.txt"), "w", encoding="utf-8") as f:
          f.write("\n".join(nlp_text))

  # Iterate over all XML files in the directory
  for file_name in os.listdir(input_dir):
      if file_name.endswith(".xml"):
          file_path = os.path.join(input_dir, file_name)
          print(f"Processing {file_name}...")
          process_xml_file(file_path)

  print("Processing complete. Files saved in:", output_dir)

#+end_src

**** Punctuation included - verse by verse

#+begin_src python
import xml.etree.ElementTree as ET
import re
import os

# Define input and output directories
input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"
output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct"
# Ensure output directory exists
os.makedirs(output_dir, exist_ok=True)

# Function to process a single XML file
def process_xml_file(file_path):
    tree = ET.parse(file_path)
    root = tree.getroot()
    
    verses = []
    current_verse = []
    
    for paragraph in root.findall(".//p"):
        for element in paragraph:
            if element.tag == "verse-number":
                if current_verse:  # If there's already a verse collected, store it
                    verses.append("".join(current_verse).strip())
                    current_verse = []  # Start a new verse
                
            if element.tag in ["w", "prefix", "suffix"]:
                text = element.text.strip() if element.text else ""
                if text:
                    if element.tag == "w" and current_verse:  # Add a space before words, except the first
                        current_verse.append(" ")
                    current_verse.append(text)
    
    # Capture the last verse
    if current_verse:
        verses.append("".join(current_verse).strip())

    # Clean text: keep Greek letters, spaces, and punctuation marks (.,;¬∑)
    verses = [re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", verse) for verse in verses]
    
    # Extract filename without extension
    base_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Save NLP-ready text
    with open(os.path.join(output_dir, f"{base_name}_nlp_ready.txt"), "w", encoding="utf-8") as f:
        f.write("\n".join(verses))

# Iterate over all XML files in the directory
for file_name in os.listdir(input_dir):
    if file_name.endswith(".xml"):
        file_path = os.path.join(input_dir, file_name)
        print(f"Processing {file_name}...")
        process_xml_file(file_path)

print("Processing complete. Files saved in:", output_dir)
#+end_src

** Swete_LXX
- From [[https://github.com/OpenGreekAndLatin/septuagint-dev][Github, septuagint-dev]]
*** Script for extraction according to Book, resulting in human-readable text & NLP-friendly text - no punct
#+begin_src python
import xml.etree.ElementTree as ET
import re
import os
import json

# Define input XML file and output directory
xml_file_path = "old_testament_1901_vol1.xml"  # Change to your actual file
output_dir = "processed_books"
os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists

# Define TEI XML namespace
NAMESPACE = {"tei": "http://www.tei-c.org/ns/1.0"}

# Function to clean and extract only Ancient Greek text
def extract_greek_text(text):
    """Removes all non-Greek characters while preserving diacritics and brackets."""
    text = re.sub(r"<.*?>", "", text)  # Remove XML-like tags
    return text.strip()

# Function to extract bracketed text as variants
def extract_variants(text):
    """Finds bracketed text and returns a list of text-critical variants."""
    variants = re.findall(r"\[(.*?)\]", text)  # Extract text inside brackets
    return variants

# Load and parse the XML file
tree = ET.parse(xml_file_path)
root = tree.getroot()

# Process each book separately
for book in root.findall(".//tei:div[@type='book']", NAMESPACE):
    book_title_element = book.find("tei:head", NAMESPACE)  # Find book title
    book_title = book_title_element.text.strip() if book_title_element is not None else "Unknown_Book"
    
    # File names
    book_filename_clean = f"{book_title.replace(' ', '_')}_nlp_ready.txt"
    book_filename_variants = f"{book_title.replace(' ', '_')}_variants.jsonl"
    book_filename_marginal_notes = f"{book_title.replace(' ', '_')}_marginal_notes.jsonl"
    book_filename_human_readable = f"{book_title.replace(' ', '_')}_human_readable.txt"

    book_texts = []
    human_readable_texts = []
    variant_entries = []
    marginal_note_entries = []

    # Extract Greek text from paragraphs within the book
    for paragraph in book.findall(".//tei:p", NAMESPACE):
        paragraph_text = "".join(paragraph.itertext())  # Get full paragraph text
        cleaned_text = extract_greek_text(paragraph_text)
        
        # Extract variants before removing brackets
        variants = extract_variants(cleaned_text)
        
        # Store variants separately with context
        if variants:
            variant_entries.append(json.dumps({
                "book": book_title,
                "context": paragraph_text.replace("\n", " "),  # Store full text for reference
                "variants": variants
            }, ensure_ascii=False))

        # Remove bracketed text for the clean NLP version
        cleaned_text_nlp = re.sub(r"\[.*?\]", "", cleaned_text).strip()
        if cleaned_text_nlp:
            book_texts.append(cleaned_text_nlp)

        # Add paragraph to the human-readable version, keeping brackets for variants
        human_readable_texts.append(paragraph_text)

    # Extract marginal notes (excluding footnotes)
    for note in book.findall(".//tei:note", NAMESPACE):
        note_type = note.get("type", "")
        if note_type == "footnote":  # Ignore footnotes
            continue
        
        note_text = "".join(note.itertext()).strip()  # Get the text inside <note>
        cleaned_note = extract_greek_text(note_text)
        
        if cleaned_note:
            marginal_note_entries.append(json.dumps({
                "book": book_title,
                "note": cleaned_note
            }, ensure_ascii=False))

    # Save cleaned Greek text as a .txt file
    with open(os.path.join(output_dir, book_filename_clean), "w", encoding="utf-8") as f:
        f.write("\n".join(book_texts))

    # Save human-readable text with references
    with open(os.path.join(output_dir, book_filename_human_readable), "w", encoding="utf-8") as f:
        f.write("\n".join(human_readable_texts))

    # Save variants as a JSONL file (one JSON object per line)
    if variant_entries:
        with open(os.path.join(output_dir, book_filename_variants), "w", encoding="utf-8") as f:
            f.write("\n".join(variant_entries) + "\n")

    # Save marginal notes as a JSONL file (one JSON object per line)
    if marginal_note_entries:
        with open(os.path.join(output_dir, book_filename_marginal_notes), "w", encoding="utf-8") as f:
            f.write("\n".join(marginal_note_entries) + "\n")

    print(f"Processed: {book_title}")

print("Processing complete! Cleaned texts, variants, and marginal notes saved separately.")
#+end_src
*** Same, but punctuation included
#+begin_src python :results output
    import xml.etree.ElementTree as ET
    import re
    import os
    import json
    from glob import glob  # Allows processing multiple XML files
    
    # Define input and output directories
    xml_files = glob("/home/gnosis/Documents/au_work/main/corpora/opensource/swete_sep/*.xml")
    # Folder containing the split XML parts
    output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/Swete_punct"
    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists

    # Define TEI XML namespace
    NAMESPACE = {"tei": "http://www.tei-c.org/ns/1.0"}

    # Function to clean and extract only Ancient Greek text (preserving punctuation)
    def extract_greek_text(text):
        """Removes all non-Greek characters except punctuation and diacritics."""
        return re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑()\[\]]", "", text).strip()

    # Function to extract bracketed text as variants
    def extract_variants(text):
        """Finds bracketed text and returns a list of text-critical variants."""
        return re.findall(r"\[(.*?)\]", text)  # Extract text inside brackets

    # Process each XML file separately
    for xml_file_path in xml_files:
        print(f"Processing: {xml_file_path}")
        
        # Load and parse the XML file
        tree = ET.parse(xml_file_path)
        root = tree.getroot()

        # Process each book separately
        for book in root.findall(".//tei:div[@type='book']", NAMESPACE):
            book_title_element = book.find("tei:head", NAMESPACE)  # Find book title
            book_title = book_title_element.text.strip() if book_title_element is not None else "Unknown_Book"

            # File names
            book_filename_clean = f"{book_title.replace(' ', '_')}_nlp_ready.txt"
            book_filename_variants = f"{book_title.replace(' ', '_')}_variants.jsonl"
            book_filename_marginal_notes = f"{book_title.replace(' ', '_')}_marginal_notes.jsonl"
            book_filename_human_readable = f"{book_title.replace(' ', '_')}_human_readable.txt"

            book_texts = []
            human_readable_texts = []
            variant_entries = []
            marginal_note_entries = []

            # Extract Greek text from paragraphs within the book
            for paragraph in book.findall(".//tei:p", NAMESPACE):
                verse_text = []
                
                # Iterate over elements inside <p> (words & punctuation)
                for element in paragraph:
                    if element.tag in ["{http://www.tei-c.org/ns/1.0}lb", "{http://www.tei-c.org/ns/1.0}pb"]:
                        # Preserve line breaks and page breaks for human-readable text
                        verse_text.append(f"[{element.tag} {element.attrib.get('n', '?')}] ")
                    elif element.tag == "{http://www.tei-c.org/ns/1.0}note":
                        # Handle text-critical notes separately
                        note_type = element.attrib.get("type", "")
                        note_text = "".join(element.itertext()).strip()
                        cleaned_note = extract_greek_text(note_text)

                        if cleaned_note:
                            if note_type == "marginal":
                                marginal_note_entries.append(json.dumps({
                                    "book": book_title,
                                    "note": cleaned_note
                                }, ensure_ascii=False))
                            elif note_type == "footnote":
                                variant_entries.append(json.dumps({
                                    "book": book_title,
                                    "context": "Footnote",
                                    "variants": [cleaned_note]
                                }, ensure_ascii=False))
                    elif element.tag == "{http://www.tei-c.org/ns/1.0}milestone":
                        # Capture verse references (e.g., stored in parentheses or brackets)
                        verse_ref = element.attrib.get("n", "?")
                        verse_text.append(f"({verse_ref}) ")
                    else:
                        # Extract text (words & punctuation together)
                        text = "".join(element.itertext()).strip()
                        if text:
                            verse_text.append(text)

                # Join words and punctuation properly
                full_text = " ".join(verse_text).strip()
                cleaned_text = extract_greek_text(full_text)
                
                # Extract text-critical variants
                variants = extract_variants(cleaned_text)
                
                # Store variants separately with context
                if variants:
                    variant_entries.append(json.dumps({
                        "book": book_title,
                        "context": full_text,
                        "variants": variants
                    }, ensure_ascii=False))

                # Remove bracketed text for the clean NLP version
                cleaned_text_nlp = re.sub(r"\[.*?\]", "", cleaned_text).strip()
                if cleaned_text_nlp:
                    book_texts.append(cleaned_text_nlp)

                # Add paragraph to the human-readable version, keeping brackets for variants
                human_readable_texts.append(full_text)

            # Append results to existing files (since XML is split into parts)
            with open(os.path.join(output_dir, book_filename_clean), "a", encoding="utf-8") as f:
                f.write("\n".join(book_texts) + "\n")

            with open(os.path.join(output_dir, book_filename_human_readable), "a", encoding="utf-8") as f:
                f.write("\n".join(human_readable_texts) + "\n")

            if variant_entries:
                with open(os.path.join(output_dir, book_filename_variants), "a", encoding="utf-8") as f:
                    f.write("\n".join(variant_entries) + "\n")

            if marginal_note_entries:
                with open(os.path.join(output_dir, book_filename_marginal_notes), "a", encoding="utf-8") as f:
                    f.write("\n".join(marginal_note_entries) + "\n")

            print(f"Processed: {book_title}")

    print("Processing complete! Cleaned texts, variants, and marginal notes saved separately.")
#+end_src

#+RESULTS:
: Processing: /home/gnosis/Documents/au_work/main/corpora/opensource/swete_sep/old_testament_1901_vol1.xml
: Processing: /home/gnosis/Documents/au_work/main/corpora/opensource/swete_sep/old_testament_1891_vol2.xml
: Processing: /home/gnosis/Documents/au_work/main/corpora/opensource/swete_sep/old_testament_1930_vol3.xml
: Processing complete! Cleaned texts, variants, and marginal notes saved separately.


*** Same, but punctuation included
#+begin_src python :results output
  import xml.etree.ElementTree as ET
  import re
  import os
  import json
  from glob import glob  # Allows processing multiple XML files

  # Define input and output directories
  xml_files = glob("/home/gnosis/Documents/au_work/main/corpora/opensource/swete_sep/*.xml")
  # Folder containing the split XML parts
  output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/Swete_punct"
  os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists

  # Define TEI XML namespace
  NAMESPACE = {"tei": "http://www.tei-c.org/ns/1.0"}

  # Function to clean and extract only Ancient Greek text (preserving punctuation)
  def extract_greek_text(text):
      """Removes all non-Greek characters except punctuation and diacritics."""
      return re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑()\[\]]", "", text).strip()

  # Function to extract bracketed text as variants
  def extract_variants(text):
      """Finds bracketed text and returns a list of text-critical variants."""
      return re.findall(r"\[(.*?)\]", text)  # Extract text inside brackets

  # Process each XML file separately
  for xml_file_path in xml_files:
      print(f"Processing: {xml_file_path}")
      
      # Load and parse the XML file
      tree = ET.parse(xml_file_path)
      root = tree.getroot()

      # Process each chapter separately
      for chapter in root.findall(".//tei:div[@type='textpart'][@subtype='chapter']", NAMESPACE):
          # Extract book title from <head>
          book_title_element = chapter.find("tei:head", NAMESPACE)
          book_title = book_title_element.text.strip() if book_title_element is not None else "Unknown_Book"

          # File names
          book_filename_clean = os.path.join(output_dir, f"{book_title.replace(' ', '_')}_nlp_ready.txt")
          book_filename_variants = os.path.join(output_dir, f"{book_title.replace(' ', '_')}_variants.jsonl")
          book_filename_marginal_notes = os.path.join(output_dir, f"{book_title.replace(' ', '_')}_marginal_notes.jsonl")
          book_filename_human_readable = os.path.join(output_dir, f"{book_title.replace(' ', '_')}_human_readable.txt")

          book_texts = []
          human_readable_texts = []
          variant_entries = []
          marginal_note_entries = []

          # Extract Greek text from paragraphs within the chapter
          for paragraph in chapter.findall(".//tei:p", NAMESPACE):
              verse_text = []
              chapter_number = None

              # Iterate over elements inside <p> (words, punctuation, references)
              for element in paragraph:
                  if element.tag == "{http://www.tei-c.org/ns/1.0}lb":
                      # Preserve verse numbers
                      verse_text.append(f"[{element.attrib.get('n', '?')}] ")
                  elif element.tag == "{http://www.tei-c.org/ns/1.0}note":
                      note_type = element.attrib.get("type", "")
                      note_text = "".join(element.itertext()).strip()
                      cleaned_note = extract_greek_text(note_text)

                      if note_type == "marginal":
                          # Some marginal notes contain chapter numbers in Roman numerals
                          if re.match(r"^[IVXLCDM]+$", cleaned_note):  
                              chapter_number = cleaned_note  # Store Roman numeral chapter
                          marginal_note_entries.append(json.dumps({
                              "book": book_title,
                              "note": cleaned_note
                          }, ensure_ascii=False))
                      elif note_type == "footnote":
                          variant_entries.append(json.dumps({
                              "book": book_title,
                              "context": "Footnote",
                              "variants": [cleaned_note]
                          }, ensure_ascii=False))
                  else:
                      # Extract main text
                      text = "".join(element.itertext()).strip()
                      if text:
                          verse_text.append(text)

              # Join words and punctuation properly
              full_text = " ".join(verse_text).strip()
              cleaned_text = extract_greek_text(full_text)
              
              # Extract text-critical variants
              variants = extract_variants(cleaned_text)
              
              # Store variants separately with context
              if variants:
                  variant_entries.append(json.dumps({
                      "book": book_title,
                      "context": full_text,
                      "variants": variants
                  }, ensure_ascii=False))

              # Remove bracketed text for the clean NLP version
              cleaned_text_nlp = re.sub(r"\[.*?\]", "", cleaned_text).strip()
              if cleaned_text_nlp:
                  book_texts.append(cleaned_text_nlp)

              # Format human-readable text with chapter & verse info
              formatted_text = f"{book_title} {chapter_number if chapter_number else ''} {full_text}"
              human_readable_texts.append(formatted_text)

          # **Write output files**
          if book_texts:
              with open(book_filename_clean, "a", encoding="utf-8") as f:
                  f.write("\n".join(book_texts) + "\n")
              print(f"Saved: {book_filename_clean}")

          if human_readable_texts:
              with open(book_filename_human_readable, "a", encoding="utf-8") as f:
                  f.write("\n".join(human_readable_texts) + "\n")
              print(f"Saved: {book_filename_human_readable}")

          if variant_entries:
              with open(book_filename_variants, "a", encoding="utf-8") as f:
                  f.write("\n".join(variant_entries) + "\n")
              print(f"Saved: {book_filename_variants}")

          if marginal_note_entries:
              with open(book_filename_marginal_notes, "a", encoding="utf-8") as f:
                  f.write("\n".join(marginal_note_entries) + "\n")
              print(f"Saved: {book_filename_marginal_notes}")

          print(f"Processed: {book_title}")

  print("Processing complete! Cleaned texts, variants, and marginal notes saved separately.")
#+end_src

** Eliranwong - LXX, Rahlf
- From [[https://github.com/eliranwong/LXX-Rahlfs-1935/tree/master/11_end-users_files/MyBible/Bibles][Github, "Mybibles", final main & books]]

- Firstly - what are the various books, which are contained in this huge .csv?
#+begin_src csv
#ccccff	10	Gen	Genesis
#ccccff	20	Exod	Exodus
#ccccff	30	Lev	Leviticus
#ccccff	40	Num	Numbers
#ccccff	50	Deut	Deuteronomy
#ffcc99	60	JoshB	Joshua B
#ffcc99	70	JudgB	Judges B
#ffcc99	80	Ruth	Ruth
#ffcc99	90	1Sam	1 Samuel (1 Kingdoms)
#ffcc99	100	2Sam	2 Samuel (2 Kingdoms)
#ffcc99	110	1Kgs	1 Kings (3 Kingdoms)
#ffcc99	120	2Kgs	2 Kings (4 Kingdoms)
#ffcc99	130	1Chr	1 Chronicles
#ffcc99	140	2Chr	2 Chronicles
#ffcc99	150	Ezra	Ezra (Esdras B/II: 1-10)
#ffcc99	160	Neh	Nehemiah (Esdras B/II: 11-23)
#ffcc99	190	Esth	Esther (with additions)
#66ff99	220	Job	Job
#66ff99	230	Ps	Psalms
#66ff99	240	Prov	Proverbs
#66ff99	250	Qoh	Ecclesiastes (Preacher)
#66ff99	260	Cant	Canticle (Song of Solomon)
#ff9fb4	290	Isa	Isaiah
#ff9fb4	300	Jer	Jeremiah
#ff9fb4	310	Lam	Lamentations (Threni)
#ff9fb4	330	Ezek	Ezekiel
#ff9fb4	340	DanOG	Daniel LXX
#ffff99	350	Hos	Hosea
#ffff99	360	Joel	Joel
#ffff99	370	Amos	Amos
#ffff99	380	Obad	Obadiah
#ffff99	390	Jonah	Jonah
#ffff99	400	Mic	Micah
#ffff99	410	Nah	Nahum
#ffff99	420	Hab	Habakkuk
#ffff99	430	Zeph	Zephaniah
#ffff99	440	Hag	Haggai
#ffff99	450	Zech	Zechariah
#ffff99	460	Mal	Malachi
#C0C0C0	165	1Esdr	Esdras A/I
#C0C0C0	170	TobBA	Tobit BA
#C0C0C0	180	Jdt	Judith
#C0C0C0	232	PsSol	Psalms of Solomon
#C0C0C0	462	1Mac	I Maccabees
#C0C0C0	464	2Mac	II Maccabees
#C0C0C0	466	3Mac	III Maccabees
#C0C0C0	467	4Mac	IV Maccabees
#C0C0C0	270	Wis	Wisdom of Solomon
#C0C0C0	280	Sir	Wisdom of Sirach
#C0C0C0	315	EpJer	Epistle of Jeremiah
#C0C0C0	320	Bar	Baruch
#C0C0C0	325	SusOG	Susanna LXX
#C0C0C0	345	BelOG	Bel LXX
#C0C0C0	800	Od	Odes
#+end_src
*** Script for extraction according to Book, resulting in human-readable text & NLP-friendly text
#+begin_src python
  import pandas as pd
  import re
  import os

  # Load the CSV file
  file_path = "/home/gnosis/Documents/au_work/main/corpora/opensource/rahfl_eliranwong/LXX_final_main.csv"
  df = pd.read_csv(file_path, encoding="utf-8")

  # Output directory
  output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/Rahfl_LXX"
  os.makedirs(output_dir, exist_ok=True)  # Ensure the folder exists

  # Mapping book codes to names
  book_mapping = {
      "10": "Genesis", "20": "Exodus", "30": "Leviticus", "40": "Numbers", "50": "Deuteronomy",
      "60": "Joshua_B", "70": "Judges_B", "80": "Ruth", "90": "1_Samuel", "100": "2_Samuel",
      "110": "1_Kings", "120": "2_Kings", "130": "1_Chronicles", "140": "2_Chronicles", "150": "Ezra",
      "160": "Nehemiah", "190": "Esther", "220": "Job", "230": "Psalms", "240": "Proverbs",
      "250": "Ecclesiastes", "260": "Song_of_Solomon", "290": "Isaiah", "300": "Jeremiah",
      "310": "Lamentations", "330": "Ezekiel", "340": "Daniel_LXX", "350": "Hosea", "360": "Joel",
      "370": "Amos", "380": "Obadiah", "390": "Jonah", "400": "Micah", "410": "Nahum",
      "420": "Habakkuk", "430": "Zephaniah", "440": "Haggai", "450": "Zechariah", "460": "Malachi",
      "165": "1_Esdras", "170": "Tobit_BA", "180": "Judith", "232": "Psalms_of_Solomon",
      "462": "1_Maccabees", "464": "2_Maccabees", "466": "3_Maccabees", "467": "4_Maccabees",
      "270": "Wisdom_of_Solomon", "280": "Wisdom_of_Sirach", "315": "Epistle_of_Jeremiah",
      "320": "Baruch", "325": "Susanna_LXX", "345": "Bel_LXX", "800": "Odes"
  }

  # Initialize storage for books
  human_readable_books = {book: [] for book in book_mapping.values()}
  nlp_text_books = {book: [] for book in book_mapping.values()}

  # Process each row
  for row in df.iloc[:, 0]:
      parts = row.split("\t")
      if len(parts) < 4:
          continue  # Skip malformed rows

      book_code, chapter, verse = parts[:3]
      greek_text = re.sub(r"<S>\d+</S>|<m>.*?</m>", "", parts[3])  # Remove markers
      greek_text_human = re.sub(r"<.*?>", "", greek_text)  # Remove remaining XML-like markup
      greek_text_nlp = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s]", "", greek_text)  # Keep only Greek characters and diacritics

      if book_code in book_mapping:
          book_name = book_mapping[book_code]
          human_readable_books[book_name].append(f"{book_code}:{chapter}:{verse} {greek_text_human}")
          nlp_text_books[book_name].append(greek_text_nlp)
  # Save outputs in the output directory
  for book, content in human_readable_books.items():
      if content:  # Only save if content exists
          file_path = os.path.join(output_dir, f"{book}_human_readable.txt")
          with open(file_path, "w", encoding="utf-8") as f:
              f.write("\n".join(content))
          print(f"Saved: {file_path}")

  for book, content in nlp_text_books.items():
      if content:  # Only save if content exists
          file_path = os.path.join(output_dir, f"{book}_nlp_ready.txt")
          with open(file_path, "w", encoding="utf-8") as f:
              f.write("\n".join(content))
          print(f"Saved: {file_path}")

  print(f"Processing complete! Files saved in '{output_dir}'")
#+end_src

#+RESULTS:
: None
** Brenton, LXX - USFX
*** Script for extraction resulting in human-readable text & NLP-friendly text - with punctuation

#+begin_src python :results output
  from lxml import etree
  import re
  import os

  # Define input USFX file
  usfx_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/grcbrent_usfx.xml"  # input usfx

  # Define output directories
  output_dir_nlp = "/home/gnosis/Documents/au_work/main/corpora/extract/brent_punct"
  output_dir_human = "/home/gnosis/Documents/au_work/main/corpora/extract/brent_punct_human"

  # Ensure output directories exist
  os.makedirs(output_dir_nlp, exist_ok=True)
  os.makedirs(output_dir_human, exist_ok=True)


  # Function to extract Greek text while keeping punctuation
  def extract_greek_text(text):
      cleaned = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", text).strip()
      return cleaned

  # # Book name mapping
  # book_mapping = {
  #     "GEN": "ŒìŒïŒùŒïŒ£ŒôŒ£", "EXO": "ŒïŒûŒüŒîŒüŒ£", "LEV": "ŒõŒïŒ•ŒôŒ§ŒôŒöŒüŒù", "NUM": "ŒëŒ°ŒôŒòŒúŒüŒô",
  #     "DEU": "ŒîŒïŒ•Œ§ŒïŒ°ŒüŒùŒüŒúŒôŒüŒù", "JOS": "ŒôŒóŒ£ŒüŒ•Œ£ ŒùŒëŒ•Œó", "JDG": "ŒöŒ°ŒôŒ§ŒëŒô", "RUT": "Œ°ŒüŒ•Œò",
  #     "1SA": "ŒíŒëŒ£ŒôŒõŒïŒôŒ©Œù Œë", "2SA": "ŒíŒëŒ£ŒôŒõŒïŒôŒ©Œù Œí", "1KI": "ŒíŒëŒ£ŒôŒõŒïŒôŒ©Œù Œì", "2KI": "ŒíŒëŒ£ŒôŒõŒïŒôŒ©Œù Œî",
  #     "PSA": "Œ®ŒëŒõŒúŒüŒô", "PRO": "Œ†ŒëŒ°ŒüŒôŒúŒôŒëŒô", "ECC": "ŒïŒöŒöŒõŒóŒ£ŒôŒëŒ£Œ§ŒóŒ£", "ISA": "ŒóŒ£ŒëŒôŒëŒ£",
  # }

  # Load the USFX XML file using lxml
  tree = etree.parse(usfx_file)
  root = tree.getroot()

  # Process each book
  for book in root.findall("./book"):
      book_code = book.get("id", "UNKNOWN")
      book_name = book_code  # Just use the ID as the book name
      
      book_nlp_text = []
      book_human_text = []

      print(f"üîç Processing book: {book_code} ({book_name})")

      # Process paragraphs that contain verses
      for paragraph in book.findall(".//p"):
              paragraph_nlp = []
              paragraph_human = []

              for verse in paragraph.findall(".//v"):
                  verse_bcv = verse.get("bcv", "?")  # Example: JON.1.1
                  verse_parts = verse_bcv.split(".")  # Split into ["JON", "1", "1"]

                  # Convert to "1:1" format if properly structured
                  if len(verse_parts) == 3:
                      verse_num = f"{verse_parts[1]}:{verse_parts[2]}"
                  else:
                      verse_num = verse_bcv  # Fallback in case of incorrect format

                  # Extract text from the verse node and its tail
                  raw_text = (verse.text or "") + (verse.tail or "")

                  if not raw_text.strip():
                      print(f"‚ö†Ô∏è Warning: No text found for verse {verse_num} in {book_name}")

                  cleaned_text = extract_greek_text(raw_text)

                  if cleaned_text:
                      paragraph_nlp.append(cleaned_text)
                      paragraph_human.append(f"{verse_num} {cleaned_text}")
                  else:
                      print(f"‚ö†Ô∏è Empty cleaned text for {verse_num} in {book_name}")

                  # Handle <ve /> nodes for human-readable version
                  next_node = verse.getnext()
                  if next_node is not None and next_node.tag == "ve":
                      pass
                      # paragraph_human.append("\n")  # Insert line break after a verse

              # Add processed paragraph text
              if paragraph_nlp:
                  book_nlp_text.append(" ".join(paragraph_nlp))  # Keep NLP-friendly text per paragraph
              if paragraph_human:
                  book_human_text.extend(paragraph_human)  # Keep human-readable format with verse numbers


      # **Write output once per book**
      if book_nlp_text:
          print(f"üìù Writing {len(book_nlp_text)} NLP-ready lines for {book_name}")
          print("NLP content example:", book_nlp_text[:3])  # Print first 3 lines

          output_file_nlp = os.path.join(output_dir_nlp, f"{book_name}_nlp_ready.txt")
          with open(output_file_nlp, "w", encoding="utf-8") as f:
              f.write("\n".join(book_nlp_text) + "\n")  # Ensure new line at end of file
          print(f"‚úÖ Saved NLP version: {output_file_nlp}")

      if book_human_text:
          print(f"üìñ Writing {len(book_human_text)} human-readable lines for {book_name}")
          print("Human-readable content example:", book_human_text[:3])  # Print first 3 lines

          output_file_human = os.path.join(output_dir_human, f"{book_name}_human_readable.txt")
          with open(output_file_human, "w", encoding="utf-8") as f:
              f.write("\n".join(line.strip() for line in book_human_text))
          print(f"‚úÖ Saved human-friendly version: {output_file_human}")

  print(f"üéâ Processing complete! Check the output folders:\n- NLP Ready: {output_dir_nlp}\n- Human Readable: {output_dir_human}")
#+end_src

** Brenton & SBLGNT - "."-separated
#+begin_src python
import os
import re
import xml.etree.ElementTree as ET
from lxml import etree

# Define input directories
sblgnt_input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"
brenton_usfx_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/grcbrent_usfx.xml"

# Define main output directory
main_output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/nlp/punct"

# Ensure subdirectories exist
os.makedirs(main_output_dir, exist_ok=True)

# Automatically create separate subdirectories

sblgnt_output_dir = os.path.join(main_output_dir, "SBLGNT")
brenton_output_dir = os.path.join(main_output_dir, "Brenton")

# Ensure subdirectories exist
os.makedirs(sblgnt_output_dir, exist_ok=True)
os.makedirs(brenton_output_dir, exist_ok=True)

# Function to clean and split text (only on '.')
def clean_and_split_text(text):
    # Keep only Greek characters, spaces, and punctuation
    cleaned_text = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", text)

    # Fix spaces around punctuation: Remove spaces before , . ; ¬∑ but keep after
    cleaned_text = re.sub(r"\s*([,.;¬∑])\s*", r"\1 ", cleaned_text)

    # Ensure we split at periods but retain them
    sentences = [s.strip() + "." if s and not s.endswith(".") else s.strip() for s in cleaned_text.split(".") if s.strip()]
    
    return sentences

# Process SBLGNT XML files
def process_sblgnt_xml(file_path, output_dir):
    tree = ET.parse(file_path)
    root = tree.getroot()
    book_sentences = []
    
    for paragraph in root.findall(".//p"):
        verse_text = [element.text.strip() for element in paragraph if element.tag in ["w", "prefix", "suffix"] and element.text]
        full_text = " ".join(verse_text)
        book_sentences.extend(clean_and_split_text(full_text))
    
    output_file = os.path.join(output_dir, os.path.basename(file_path).replace(".xml", "_nlp_ready.txt"))
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(book_sentences))
    print(f"‚úÖ SBLGNT saved: {output_file}")

# Process Brenton USFX XML
def process_brenton_usfx(file_path, output_dir):
    tree = etree.parse(file_path)
    root = tree.getroot()
    
    for book in root.findall("./book"):
        book_code = book.get("id", "UNKNOWN")
        book_sentences = []
        
        for paragraph in book.findall(".//p"):
            paragraph_text = []
            
            for verse in paragraph.findall(".//v"):
                raw_text = (verse.text or "") + (verse.tail or "")
                paragraph_text.append(raw_text)
            
            full_text = " ".join(paragraph_text)
            book_sentences.extend(clean_and_split_text(full_text))
        
        output_file = os.path.join(output_dir, f"{book_code}_nlp_ready.txt")
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("\n".join(book_sentences))
        print(f"‚úÖ Brenton saved: {output_file}")

# Run extraction for SBLGNT
for file_name in os.listdir(sblgnt_input_dir):
    if file_name.endswith(".xml"):
        process_sblgnt_xml(os.path.join(sblgnt_input_dir, file_name), sblgnt_output_dir)

# Run extraction for Brenton
process_brenton_usfx(brenton_usfx_file, brenton_output_dir)

print("üéâ Extraction complete!")
#+end_src

#+RESULTS:
: None

** Brenton & SBLGNT - continuous string
#+begin_src python
  import os
  import re
  import xml.etree.ElementTree as ET
  from lxml import etree

  # Define input directories
  sblgnt_input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"
  brenton_usfx_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/grcbrent_usfx.xml"

  # Define main output directory
  main_output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/nlp/continuous"

  # Ensure subdirectories exist
  os.makedirs(main_output_dir, exist_ok=True)

  # Automatically create separate subdirectories
  sblgnt_output_dir = os.path.join(main_output_dir, "SBLGNT")
  brenton_output_dir = os.path.join(main_output_dir, "Brenton")

  # Ensure subdirectories exist
  os.makedirs(sblgnt_output_dir, exist_ok=True)
  os.makedirs(brenton_output_dir, exist_ok=True)

  # Function to clean text (without splitting into sentences)
  def clean_text(text):
      # Keep only Greek characters, spaces, and punctuation
      cleaned_text = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", text)
      # Fix spaces around punctuation: Remove spaces before , . ; ¬∑ but keep after
      cleaned_text = re.sub(r"\s*([,.;¬∑])\s*", r"\1 ", cleaned_text)
      return cleaned_text.strip()

  # Process SBLGNT XML files
  def process_sblgnt_xml(file_path, output_dir):
      tree = ET.parse(file_path)
      root = tree.getroot()
      full_text = ""
      
      for paragraph in root.findall(".//p"):
          verse_text = [element.text.strip() for element in paragraph if element.tag in ["w", "prefix", "suffix"] and element.text]
          full_text += " " + clean_text(" ".join(verse_text))
      
      output_file = os.path.join(output_dir, os.path.basename(file_path).replace(".xml", "_nlp_ready.txt"))
      with open(output_file, "w", encoding="utf-8") as f:
          f.write(full_text.strip())
      print(f"‚úÖ SBLGNT saved: {output_file}")

  # Process Brenton USFX XML
  def process_brenton_usfx(file_path, output_dir):
      tree = etree.parse(file_path)
      root = tree.getroot()
      
      for book in root.findall("./book"):
          book_code = book.get("id", "UNKNOWN")
          full_text = ""
          
          for paragraph in book.findall(".//p"):
              paragraph_text = []
              
              for verse in paragraph.findall(".//v"):
                  raw_text = (verse.text or "").strip() + " " + (verse.tail or "").strip()
                  paragraph_text.append(raw_text.strip())
              
              full_text += " " + clean_text(" ".join(paragraph_text)).replace("\n", " ")
          
          output_file = os.path.join(output_dir, f"{book_code}_nlp_ready.txt")
          with open(output_file, "w", encoding="utf-8") as f:
              f.write(full_text.strip())
          print(f"‚úÖ Brenton saved: {output_file}")
          
  # Extract text from SBLGNT
  for file_name in os.listdir(sblgnt_input_dir):
      if file_name.endswith(".xml"):
          process_sblgnt_xml(os.path.join(sblgnt_input_dir, file_name), sblgnt_output_dir)

  # Extract text from Brenton
  process_brenton_usfx(brenton_usfx_file, brenton_output_dir)

  print("üéâ Extraction complete!")
#+end_src

#+RESULTS:
: None

** Brenton & SBLGNT - sentences as per grc_proiel_trf parser
#+begin_src python
import os
import re
import xml.etree.ElementTree as ET
from lxml import etree
import spacy

MAX_LENGTH = 10**4

# Load spaCy model for Ancient Greek
nlp = spacy.load("grc_proiel_trf")

# Define input directories
sblgnt_input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"
brenton_usfx_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/grcbrent_usfx.xml"

# Define main output directory
main_output_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/nlp/sentences"
os.makedirs(main_output_dir, exist_ok=True)
sblgnt_output_dir = os.path.join(main_output_dir, "SBLGNT")
brenton_output_dir = os.path.join(main_output_dir, "Brenton")
os.makedirs(sblgnt_output_dir, exist_ok=True)
os.makedirs(brenton_output_dir, exist_ok=True)

# Function to clean text while preserving punctuation
def clean_text(text):
    # Keep Greek characters, spaces, and punctuation
    cleaned_text = re.sub(r"[^\u0370-\u03FF\u1F00-\u1FFF\s.,;¬∑]", "", text)
    # Normalize spaces around punctuation
    cleaned_text = re.sub(r"\s*([,.;¬∑])\s*", r"\1 ", cleaned_text)
    return cleaned_text.strip()

def chunk_text(text, max_length=MAX_LENGTH):
    """
    Splits a long text into chunks of at most max_length characters.
    This implementation splits on words to avoid cutting in the middle of a word.
    """
    if len(text) <= max_length:
        return [text]

    words = text.split()
    chunks = []
    current_chunk = []
    current_length = 0

    for word in words:
        # +1 accounts for a space between words
        if current_length + len(word) + 1 > max_length:
            chunks.append(" ".join(current_chunk))
            current_chunk = [word]
            current_length = len(word)
        else:
            current_chunk.append(word)
            current_length += len(word) + 1
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    return chunks

# Function to process text using spaCy with chunking and nlp.pipe
def process_with_spacy(text):
    # Clean the text first
    text = clean_text(text)

    # If text is too long, split it into manageable chunks
    chunks = chunk_text(text, MAX_LENGTH)

    # Process the chunks using spaCy's efficient pipe method
    docs = list(nlp.pipe(chunks))

    # Extract sentences from all processed chunks
    sentences = []
    for doc in docs:
        sentences.extend([sent.text.strip() for sent in doc.sents])
    return sentences

# Process SBLGNT XML files
def process_sblgnt_xml(file_path, output_dir):
    tree = ET.parse(file_path)
    root = tree.getroot()
    full_text = ""

    for paragraph in root.findall(".//p"):
        verse_text = [element.text.strip() for element in paragraph 
                      if element.tag in ["w", "prefix", "suffix"] and element.text]
        full_text += " " + " ".join(verse_text)

    # Process text with spaCy for sentence segmentation
    sentences = process_with_spacy(full_text)

    output_file = os.path.join(output_dir, os.path.basename(file_path).replace(".xml", "_nlp_sentences.txt"))
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(sentences))

    print(f"‚úÖ SBLGNT saved: {output_file}")

def process_brenton_usfx_stream(file_path, output_dir, batch_size=5):
    tree = etree.parse(file_path)
    root = tree.getroot()

    for book in root.findall("./book"):
        book_code = book.get("id", "UNKNOWN")
        output_file = os.path.join(output_dir, f"{book_code}_nlp_sentences.txt")
        
        with open(output_file, "w", encoding="utf-8") as f_out:
            batch = []  # To collect paragraph chunks
            for paragraph in book.findall(".//p"):
                paragraph_text = []
                for verse in paragraph.findall(".//v"):
                    raw_text = (verse.text or "").strip() + " " + (verse.tail or "").strip()
                    paragraph_text.append(raw_text.strip())
                chunk = " ".join(paragraph_text)
                
                if not chunk:
                    continue

                # Optionally clean the chunk if desired:
                chunk = clean_text(chunk)
                
                batch.append(chunk)

                # When the batch is full, process it with nlp.pipe for efficiency.
                if len(batch) >= batch_size:
                    docs = list(nlp.pipe(batch))
                    for doc in docs:
                        for sent in doc.sents:
                            sentence = sent.text.strip()
                            if sentence:
                                f_out.write(sentence + "\n")
                    batch = []  # Reset batch

            # Process any remaining chunks in the batch
            if batch:
                docs = list(nlp.pipe(batch))
                for doc in docs:
                    for sent in doc.sents:
                        sentence = sent.text.strip()
                        if sentence:
                            f_out.write(sentence + "\n")
        
        print(f"‚úÖ Processed and saved: {output_file}")
# # Extract text from SBLGNT
# for file_name in os.listdir(sblgnt_input_dir):
#     if file_name.endswith(".xml"):
#         process_sblgnt_xml(os.path.join(sblgnt_input_dir, file_name), sblgnt_output_dir)

# Extract text from Brenton
process_brenton_usfx_stream(brenton_usfx_file, brenton_output_dir)

print("üéâ Sentence extraction complete!")
#+end_src


** Plutarch

*** Extracting text, huma-readable format
#+begin_src python :results output
  from lxml import etree
  import os

  # Define the TEI namespace
  NS = {'tei': 'http://www.tei-c.org/ns/1.0'}

  def extract_text_from_element(elem):
      """
      Extracts all text from the element (and its descendants) and joins it into a single string.
      """
      # This gathers all text nodes under elem
      return " ".join(elem.xpath(".//text()", namespaces=NS)).strip()

  def process_tei_file(input_xml, output_machine, output_human):
      # Parse the TEI XML file
      tree = etree.parse(input_xml)
      root = tree.getroot()
      
      machine_output = []  # For machine-readable text (plain text)
      human_output = []    # For human-readable text (with chapter/section markers)
      
      # Locate all chapter divisions.
      # Chapters are defined as <div type="textpart" subtype="chapter" n="...">
      chapters = root.xpath("//tei:div[@type='textpart' and @subtype='chapter']", namespaces=NS)
      
      for chapter in chapters:
          chap_num = chapter.get("n", "unknown")
          # Optionally, look for a <head> element for the chapter title
          chap_head = chapter.find("tei:head", namespaces=NS)
          chap_title = chap_head.text.strip() if chap_head is not None and chap_head.text else f"Chapter {chap_num}"
          
          # Add a marker for the chapter in both outputs
          marker = f"=== {chap_title} ==="
          machine_output.append(marker)
          human_output.append(marker)
          
          # Find sections within the chapter (if any)
          sections = chapter.xpath("./tei:div[@type='textpart' and @subtype='section']", namespaces=NS)
          if sections:
              for section in sections:
                  sec_num = section.get("n", "unknown")
                  sec_head = section.find("tei:head", namespaces=NS)
                  sec_title = sec_head.text.strip() if sec_head is not None and sec_head.text else f"Section {sec_num}"
                  
                  sec_marker = f"-- {sec_title} --"
                  machine_output.append(sec_marker)
                  human_output.append(sec_marker)
                  
                  # Extract text from paragraphs in this section
                  paragraphs = section.xpath(".//tei:p", namespaces=NS)
                  sec_text = " ".join([extract_text_from_element(p) for p in paragraphs if extract_text_from_element(p)])
                  machine_output.append(sec_text)
                  human_output.append(sec_text)
          else:
              # If there are no sections, process the chapter as a whole.
              paragraphs = chapter.xpath(".//tei:p", namespaces=NS)
              chap_text = " ".join([extract_text_from_element(p) for p in paragraphs if extract_text_from_element(p)])
              machine_output.append(chap_text)
              human_output.append(chap_text)
      
      # Write the machine-readable (plain text) output to a file
      with open(output_machine, "w", encoding="utf-8") as f:
          f.write("\n".join(machine_output) + "\n")
      
      # Write the human-readable output (with chapter and section markers) to another file
      with open(output_human, "w", encoding="utf-8") as f:
          f.write("\n".join(human_output) + "\n")
      
      print(f"Machine-readable output saved to: {output_machine}")
      print(f"Human-readable output saved to: {output_human}")

  # Example usage:
  if __name__ == "__main__":
      # Replace with the path to your TEI XML file
      input_xml = "/home/gnosis/Documents/au_work/m# ain/corpora/Plutarch/raw_xml/tlg0007.tlg001.perseus-grc2.xml-ŒòŒ∑œÉŒµœçœÇ.xml"
      # Files to write the machine-readable and human-readable outputs
      output_machine = "machine_readable.txt"
      output_human = "human_readable.txt"
      
      process_tei_file(input_xml, output_machine, output_human)
#+end_src

#+RESULTS:


* Extracting names from books

** Names of books from metadata
 - Saved metadata to .csv file, filtered non-LXX & non-NT material away, now extracting the pure names
*** Extract data from csv, find title & id, overwrite file
#+begin_src python
import pandas as pd

# Define the file path
file_path = "/home/gnosis/Documents/au_work/main/corpora/cleanedlist.csv"  # Change this to your actual file path

# Read the CSV file, assuming it has a header
df = pd.read_csv(file_path, usecols=[0, 2], header=None, names=["ID", "Title(s)"])

# Overwrite the original file with the modified data
df.to_csv(file_path, index=False)

print(f"File '{file_path}' has been updated with columns: ID, Title(s)")

#+end_src

#+RESULTS:
: None

** Naming books according to ID from metadata
*** SBLGNT
Finding the Greek name for every book in order to name id after ID

**** Script
#+begin_src python :results output
import os
import xml.etree.ElementTree as ET

# Define the input directory containing XML files
input_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"  # Change this to your actual directory path

# Iterate over all XML files in the directory
for file_name in os.listdir(input_dir):
    if file_name.endswith(".xml"):
        file_path = os.path.join(input_dir, file_name)
        
        try:
            # Parse XML
            tree = ET.parse(file_path)
            root = tree.getroot()

            # Extract book title
            title_element = root.find("./title")
            title_text = title_element.text.strip() if title_element is not None else "‚ùå No title found"

            # Extract basename (without extension)
            base_name = os.path.splitext(file_name)[0]

            # Print comparison
            if base_name in title_text:
                print(f"‚úÖ Match: {file_name} -> {title_text}")
            else:
                print(f"‚ö†Ô∏è Mismatch: {file_name} -> {title_text}")

        except ET.ParseError:
            print(f"‚ùå Error parsing {file_name}")

print("\nProcessing complete!")

#+end_src

#+RESULTS:
#+begin_example
‚ö†Ô∏è Mismatch: 1Cor.xml -> Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œë
‚ö†Ô∏è Mismatch: 1John.xml -> ŒôŒ©ŒëŒùŒùŒüŒ• Œë
‚ö†Ô∏è Mismatch: 1Pet.xml -> Œ†ŒïŒ§Œ°ŒüŒ• Œë
‚ö†Ô∏è Mismatch: 1Thess.xml -> Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œë
‚ö†Ô∏è Mismatch: 1Tim.xml -> Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œë
‚ö†Ô∏è Mismatch: 2Cor.xml -> Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œí
‚ö†Ô∏è Mismatch: 2John.xml -> ŒôŒ©ŒëŒùŒùŒüŒ• Œí
‚ö†Ô∏è Mismatch: 2Pet.xml -> Œ†ŒïŒ§Œ°ŒüŒ• Œí
‚ö†Ô∏è Mismatch: 2Thess.xml -> Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œí
‚ö†Ô∏è Mismatch: 2Tim.xml -> Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œí
‚ö†Ô∏è Mismatch: 3John.xml -> ŒôŒ©ŒëŒùŒùŒüŒ• Œì
‚ö†Ô∏è Mismatch: Acts.xml -> Œ†Œ°ŒëŒûŒïŒôŒ£ ŒëŒ†ŒüŒ£Œ§ŒüŒõŒ©Œù
‚ö†Ô∏è Mismatch: Col.xml -> Œ†Œ°ŒüŒ£ ŒöŒüŒõŒüŒ£Œ£ŒëŒïŒôŒ£
‚ö†Ô∏è Mismatch: Eph.xml -> Œ†Œ°ŒüŒ£ ŒïŒ¶ŒïŒ£ŒôŒüŒ•Œ£
‚ö†Ô∏è Mismatch: Gal.xml -> Œ†Œ°ŒüŒ£ ŒìŒëŒõŒëŒ§ŒëŒ£
‚ö†Ô∏è Mismatch: Heb.xml -> Œ†Œ°ŒüŒ£ ŒïŒíŒ°ŒëŒôŒüŒ•Œ£
‚ö†Ô∏è Mismatch: Jas.xml -> ŒôŒëŒöŒ©ŒíŒüŒ•
‚ö†Ô∏è Mismatch: John.xml -> ŒöŒëŒ§Œë ŒôŒ©ŒëŒùŒùŒóŒù
‚ö†Ô∏è Mismatch: Jude.xml -> ŒôŒüŒ•ŒîŒë
‚ö†Ô∏è Mismatch: Luke.xml -> ŒöŒëŒ§Œë ŒõŒüŒ•ŒöŒëŒù
‚ö†Ô∏è Mismatch: Mark.xml -> ŒöŒëŒ§Œë ŒúŒëŒ°ŒöŒüŒù
‚ö†Ô∏è Mismatch: Matt.xml -> ŒöŒëŒ§Œë ŒúŒëŒòŒòŒëŒôŒüŒù
‚ö†Ô∏è Mismatch: Phil.xml -> Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒôŒ†Œ†ŒóŒ£ŒôŒüŒ•Œ£
‚ö†Ô∏è Mismatch: Phlm.xml -> Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒóŒúŒüŒùŒë
‚ö†Ô∏è Mismatch: Rev.xml -> ŒëŒ†ŒüŒöŒëŒõŒ•Œ®ŒôŒ£ ŒôŒ©ŒëŒùŒùŒüŒ•
‚ö†Ô∏è Mismatch: Rom.xml -> Œ†Œ°ŒüŒ£ Œ°Œ©ŒúŒëŒôŒüŒ•Œ£
‚ö†Ô∏è Mismatch: Titus.xml -> Œ†Œ°ŒüŒ£ Œ§ŒôŒ§ŒüŒù

Processing complete!
#+end_example

*** Brenton

**** Script
#+begin_src python :results output
import xml.etree.ElementTree as ET

# Path to the XML file
xml_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/BookNames.xml"  # Change this to the actual path of your file

# Parse the XML file
tree = ET.parse(xml_file)
root = tree.getroot()

# Iterate over each <book> entry and extract attributes
books_data = []
for book in root.findall("book"):
    book_info = {
        "code": book.get("code"),
        "abbr": book.get("abbr"),
        "short": book.get("short"),
        "long": book.get("long"),
        "alt": book.get("alt") if book.get("alt") else "N/A"  # Use "N/A" if alt is missing
    }
    books_data.append(book_info)

# Print results
for book in books_data:
    print(f"Code: {book['code']}")
    print(f"  Abbreviation: {book['abbr']}")
    print(f"  Short Name: {book['short']}")
    print(f"  Long Name: {book['long']}")
    print(f"  Alternative Name: {book['alt']}")
    print("-" * 40)

#+end_src
#+RESULTS:

*** Unifying

**** v1
#+begin_src python :results output
  import os
  import pandas as pd
  import xml.etree.ElementTree as ET

  # Directories and files
  sblgnt_dir = "/home/gnosis/Documents/au_work/main/corpora/opensource/SBLGNT-master/data/sblgnt/xml"
  brenton_xml_file = "/home/gnosis/Documents/au_work/main/corpora/opensource/brent_sep/grcbrent_usfx/BookNames.xml"

  # Initialize list to store data
  book_data = []

  ### PROCESS SBLGNT ###
  for file_name in os.listdir(sblgnt_dir):
      if file_name.endswith(".xml"):
          file_path = os.path.join(sblgnt_dir, file_name)

          try:
              tree = ET.parse(file_path)
              root = tree.getroot()

              # Extract book abbreviation (from <book id="">)
              book_element = root.find("./book")
              # Correct way to extract the book id
              book_abbreviation = root.get("id", "‚ùå No ID")  # Directly access the attribute


              # Extract Greek title (from <title>)
              title_element = root.find("./title")
              greek_title = title_element.text.strip() if title_element is not None else "‚ùå No title"
              # Extract file name
              base_name = os.path.splitext(file_name)[0]
              # Append to list
              book_data.append(["SBLGNT", book_abbreviation, greek_title, "N/A"])  # No "Long Title" in SBLGNT

          except ET.ParseError:
              print(f"‚ùå Error parsing {file_name}")

  ### PROCESS BRENTON LXX ###
  tree = ET.parse(brenton_xml_file)
  root = tree.getroot()

  for book in root.findall("book"):
      book_abbreviation = book.get("code", "‚ùå No Code")
      greek_title = book.get("short", "‚ùå No Short Name")
      
      long_title = book.get("long", "‚ùå No Long Name")
      alt_name = book.get("alt", "")

      # Combine Long Title and Alternative Name
      long_or_alt = long_title if not alt_name else f"{long_title} OR {alt_name}"

      # Append to list
      book_data.append(["LXX_Brenton", book_abbreviation, greek_title, long_or_alt])

  # Convert to Pandas DataFrame
  df = pd.DataFrame(book_data, columns=["Source", "Book Abbreviation", "Greek Title", "Long Title OR Alternative Name"])

  # Save to CSV
  output_file = "book_data.csv"
  df.to_csv(output_file, index=False, encoding="utf-8")

  print(f"‚úÖ Data saved to {output_file}")

#+end_src

#+RESULTS:
: ‚úÖ Data saved to book_data.csv

**** v3 - .csv produced, NEED TO CHECK THROUGH
#+begin_src python :results output
import re
import difflib
import pandas as pd

# Function to remove parenthetical parts from a string
def remove_parentheticals(text):
    return re.sub(r"\s*\(.*?\)", "", text).strip()

# Function to split a cleanedlist title string into components
def parse_cleanedlist_title(title_str):
    """
    Expected format: "English Title - Abbreviation-Greek Title (extra info)"
    Returns a dict with keys:
      - english: part before first "-"
      - candidate_abbr: first part after "-" split by "-"
      - candidate_greek: second part after "-" with parentheticals removed (if present)
    """
    parts = title_str.split("-", 1)
    if len(parts) < 2:
        return {"english": title_str.strip(), "candidate_abbr": "", "candidate_greek": ""}
    english = parts[0].strip()
    remainder = parts[1].strip()
    # Now split remainder by "-" if possible:
    rem_parts = remainder.split("-", 1)
    candidate_abbr = rem_parts[0].strip()
    candidate_greek = ""
    if len(rem_parts) > 1:
        candidate_greek = remove_parentheticals(rem_parts[1].strip())
    return {"english": english, "candidate_abbr": candidate_abbr, "candidate_greek": candidate_greek}

# Read the cleanedlist.csv (assuming no headers, with two columns)
cleanedlist_path = "/home/gnosis/Documents/au_work/main/corpora/cleanedlist.csv"
cleaned_df = pd.read_csv(cleanedlist_path, header=None, names=["Metadata_ID", "Metadata_Title"])

# Parse the cleanedlist titles and add columns for easier matching
cleaned_df[['English_Part', 'Candidate_Abbr', 'Candidate_Greek']] = cleaned_df["Metadata_Title"].apply(
    lambda t: pd.Series(parse_cleanedlist_title(t))
)

# Read the book_data.csv (which should have columns: Source, Book Abbreviation, Greek Title, Long Title OR Alternative Name)
book_data_path = "/home/gnosis/Documents/au_work/main/comp_antiquity/combined_approach/extract/book_data.csv"
book_df = pd.read_csv(book_data_path)

# Function to compute a fuzzy matching ratio
def fuzzy_ratio(a, b):
    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()

# Function to find the best match for each book entry
def find_best_match_for_book(book_row, cleaned_df, cutoff=0.6):
    best_score = 0
    best_id = None
    best_title = None  # Store matched title
    
    # Get book data candidates (all strings)
    book_abbr = str(book_row["Book Abbreviation"]).strip()
    greek_title = str(book_row["Greek Title"]).strip()
    long_alt = str(book_row["Long Title OR Alternative Name"]).strip()
    
    # Loop over cleanedlist rows:
    for _, cl_row in cleaned_df.iterrows():
        scores = []
        # Compare candidate_abbr to book_abbr
        if cl_row["Candidate_Abbr"]:
            score1 = fuzzy_ratio(cl_row["Candidate_Abbr"], book_abbr)
            scores.append(score1)
        # Compare candidate_greek to greek_title
        if cl_row["Candidate_Greek"]:
            score2 = fuzzy_ratio(cl_row["Candidate_Greek"], greek_title)
            scores.append(score2)
            # Also compare candidate_greek to long_alt if available
            score3 = fuzzy_ratio(cl_row["Candidate_Greek"], long_alt)
            scores.append(score3)
        
        if scores:
            max_score = max(scores)
            if max_score > best_score and max_score >= cutoff:
                best_score = max_score
                best_id = cl_row["Metadata_ID"]
                best_title = cl_row["Metadata_Title"]  # Store matched title
    
    return best_id, best_title, best_score

# Apply matching to each book in book_df
matched_ids = []
matched_titles = []
match_scores = []
meta_id_names = []

for _, book_row in book_df.iterrows():
    best_id, best_title, score = find_best_match_for_book(book_row, cleaned_df, cutoff=0.6)
    
    matched_ids.append(best_id if best_id is not None else "No Match")
    matched_titles.append(best_title if best_title is not None else "No Match")
    match_scores.append(score)
    
    # Look up the actual title based on best_id
    if best_id and best_id in cleaned_df["Metadata_ID"].values:
        meta_id_name = cleaned_df.loc[cleaned_df["Metadata_ID"] == best_id, "Metadata_Title"].values[0]
    else:
        meta_id_name = "No Match"
    
    meta_id_names.append(meta_id_name)

# Add the new columns to book_df
book_df["Metadata_ID"] = matched_ids
book_df["Metadata_Title"] = matched_titles
book_df["Match_Score"] = match_scores
book_df["META_ID_NAME"] = meta_id_names  # Add the new column

# Save the updated book_df to a CSV file
output_file = "matched_book_data.csv"
book_df.to_csv(output_file, index=False, encoding="utf-8")

print("Matching complete. Results saved to:", output_file)
#+end_src

#+RESULTS:
: Matching complete. Results saved to: matched_book_data.csv

* Rename

** First, fixing the name for already extracted files

*** Script for renaming files with their correct id
#+begin_src python :results output
import os

# Define the mapping of filenames to Greek titles
greek_title_map = {
    "1Cor": "Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œë",
    "1John": "ŒôŒ©ŒëŒùŒùŒüŒ• Œë",
    "1Pet": "Œ†ŒïŒ§Œ°ŒüŒ• Œë",
    "1Thess": "Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œë",
    "1Tim": "Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œë",
    "2Cor": "Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œí",
    "2John": "ŒôŒ©ŒëŒùŒùŒüŒ• Œí",
    "2Pet": "Œ†ŒïŒ§Œ°ŒüŒ• Œí",
    "2Thess": "Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œí",
    "2Tim": "Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œí",
    "3John": "ŒôŒ©ŒëŒùŒùŒüŒ• Œì",
    "Acts": "Œ†Œ°ŒëŒûŒïŒôŒ£ ŒëŒ†ŒüŒ£Œ§ŒüŒõŒ©Œù",
    "Col": "Œ†Œ°ŒüŒ£ ŒöŒüŒõŒüŒ£Œ£ŒëŒïŒôŒ£",
    "Eph": "Œ†Œ°ŒüŒ£ ŒïŒ¶ŒïŒ£ŒôŒüŒ•Œ£",
    "Gal": "Œ†Œ°ŒüŒ£ ŒìŒëŒõŒëŒ§ŒëŒ£",
    "Heb": "Œ†Œ°ŒüŒ£ ŒïŒíŒ°ŒëŒôŒüŒ•Œ£",
    "Jas": "ŒôŒëŒöŒ©ŒíŒüŒ•",
    "John": "ŒöŒëŒ§Œë ŒôŒ©ŒëŒùŒùŒóŒù",
    "Jude": "ŒôŒüŒ•ŒîŒë",
    "Luke": "ŒöŒëŒ§Œë ŒõŒüŒ•ŒöŒëŒù",
    "Mark": "ŒöŒëŒ§Œë ŒúŒëŒ°ŒöŒüŒù",
    "Matt": "ŒöŒëŒ§Œë ŒúŒëŒòŒòŒëŒôŒüŒù",
    "Phil": "Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒôŒ†Œ†ŒóŒ£ŒôŒüŒ•Œ£",
    "Phlm": "Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒóŒúŒüŒùŒë",
    "Rev": "ŒëŒ†ŒüŒöŒëŒõŒ•Œ®ŒôŒ£ ŒôŒ©ŒëŒùŒùŒüŒ•",
    "Rom": "Œ†Œ°ŒüŒ£ Œ°Œ©ŒúŒëŒôŒüŒ•Œ£",
    "Titus": "Œ†Œ°ŒüŒ£ Œ§ŒôŒ§ŒüŒù"
}

# Define the root directory containing the files
root_dir = "/home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct"  # Change this to your actual directory

# Iterate through all subdirectories and files
for dirpath, _, filenames in os.walk(root_dir):
    for file_name in filenames:
        # Extract the file prefix before the first "_"
        base_name, ext = os.path.splitext(file_name)

        # Split at "_" to check for a known book abbreviation
        parts = base_name.split("_", 1)
        if len(parts) < 2:
            continue  # Skip files that don't match the pattern
        
        book_prefix, suffix = parts  # Example: "Titus", "nlp_ready"
        
        # Check if the book prefix exists in the mapping
        if book_prefix in greek_title_map:
            new_name = f"{greek_title_map[book_prefix]}_{suffix}{ext}"  # Preserve suffix and extension
            old_path = os.path.join(dirpath, file_name)
            new_path = os.path.join(dirpath, new_name)

            # Rename the file
            os.rename(old_path, new_path)
            print(f"Renamed in {dirpath}: {file_name} -> {new_name}")

print("\n‚úÖ Renaming complete across all subdirectories!")
#+end_src

#+RESULTS:
#+begin_example
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 1Cor_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 1John_nlp_ready.txt -> ŒôŒ©ŒëŒùŒùŒüŒ• Œë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 1Pet_nlp_ready.txt -> Œ†ŒïŒ§Œ°ŒüŒ• Œë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 1Thess_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 1Tim_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 2Cor_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒöŒüŒ°ŒôŒùŒòŒôŒüŒ•Œ£ Œí_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 2John_nlp_ready.txt -> ŒôŒ©ŒëŒùŒùŒüŒ• Œí_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 2Pet_nlp_ready.txt -> Œ†ŒïŒ§Œ°ŒüŒ• Œí_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 2Thess_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒòŒïŒ£Œ£ŒëŒõŒüŒùŒôŒöŒïŒôŒ£ Œí_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 2Tim_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ§ŒôŒúŒüŒòŒïŒüŒù Œí_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: 3John_nlp_ready.txt -> ŒôŒ©ŒëŒùŒùŒüŒ• Œì_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Acts_nlp_ready.txt -> Œ†Œ°ŒëŒûŒïŒôŒ£ ŒëŒ†ŒüŒ£Œ§ŒüŒõŒ©Œù_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Col_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒöŒüŒõŒüŒ£Œ£ŒëŒïŒôŒ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Eph_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒïŒ¶ŒïŒ£ŒôŒüŒ•Œ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Gal_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒìŒëŒõŒëŒ§ŒëŒ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Heb_nlp_ready.txt -> Œ†Œ°ŒüŒ£ ŒïŒíŒ°ŒëŒôŒüŒ•Œ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Jas_nlp_ready.txt -> ŒôŒëŒöŒ©ŒíŒüŒ•_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: John_nlp_ready.txt -> ŒöŒëŒ§Œë ŒôŒ©ŒëŒùŒùŒóŒù_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Jude_nlp_ready.txt -> ŒôŒüŒ•ŒîŒë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Luke_nlp_ready.txt -> ŒöŒëŒ§Œë ŒõŒüŒ•ŒöŒëŒù_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Mark_nlp_ready.txt -> ŒöŒëŒ§Œë ŒúŒëŒ°ŒöŒüŒù_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Matt_nlp_ready.txt -> ŒöŒëŒ§Œë ŒúŒëŒòŒòŒëŒôŒüŒù_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Phil_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒôŒ†Œ†ŒóŒ£ŒôŒüŒ•Œ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Phlm_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ¶ŒôŒõŒóŒúŒüŒùŒë_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Rev_nlp_ready.txt -> ŒëŒ†ŒüŒöŒëŒõŒ•Œ®ŒôŒ£ ŒôŒ©ŒëŒùŒùŒüŒ•_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Rom_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ°Œ©ŒúŒëŒôŒüŒ•Œ£_nlp_ready.txt
Renamed in /home/gnosis/Documents/au_work/main/corpora/extract/SBLGNT_punct: Titus_nlp_ready.txt -> Œ†Œ°ŒüŒ£ Œ§ŒôŒ§ŒüŒù_nlp_ready.txt

‚úÖ Renaming complete across all subdirectories!
#+end_example
